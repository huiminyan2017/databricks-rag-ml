{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5392495-8dbf-4ac3-8a14-0d8cb9729b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07faa7d7-c26b-402e-8a53-05a20e4a661d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The * means: All parameters after * must be passed as keyword arguments.\n",
    "def embed_texts(texts, *, single: bool = None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a single string or a list of strings.\n",
    "\n",
    "    Args:\n",
    "        texts: str or list[str]\n",
    "        single: Optional[bool]. If True, returns a single embedding.\n",
    "                If False, returns a list of embeddings.\n",
    "                If None, inferred from input type.\n",
    "\n",
    "    Returns:\n",
    "        If single=True or input is str -> list[float]\n",
    "        If single=False or input is list[str] -> list[list[float]]\n",
    "    \"\"\"\n",
    "    # Normalize input\n",
    "    if isinstance(texts, str):\n",
    "        inputs = [texts]\n",
    "        inferred_single = True\n",
    "    elif isinstance(texts, list):\n",
    "        inputs = texts\n",
    "        inferred_single = False\n",
    "    else:\n",
    "        raise TypeError(\"texts must be a string or list of strings\")\n",
    "\n",
    "    if single is None:\n",
    "        single = inferred_single\n",
    "\n",
    "    response = aoai.embeddings.create(\n",
    "        model=EMBEDDING_DEPLOYMENT,\n",
    "        input=inputs\n",
    "    )\n",
    "\n",
    "    embeddings = [d.embedding for d in response.data]\n",
    "\n",
    "    if single:\n",
    "        return embeddings[0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d049aa0b-158c-4d05-b3cc-c19166aee84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cosine similarity function\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75066a79-c14e-4a98-9cc9-375eb91ad1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Option A: brute-force cosine similarity ----------\n",
    "\n",
    "def retrieve_top_k_optionA(query_embedding, k=5, limit=2000):\n",
    "\n",
    "    # Load all embedding rows\n",
    "    embeddings_df = spark.table(EMB_TABLE)\n",
    "\n",
    "    rows = embeddings_df.select(\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"chunk_text\",\n",
    "        \"category\",\n",
    "        \"embedding\"\n",
    "    ).limit(limit).collect()\n",
    "\n",
    "    scored = []\n",
    "\n",
    "    for r in rows:\n",
    "        score = cosine_similarity(query_embedding, r.embedding)\n",
    "        scored.append({\n",
    "            \"chunk_id\": r.chunk_id,\n",
    "            \"doc_id\": r.doc_id,\n",
    "            \"title\": r.title,\n",
    "            \"url\": r.url,\n",
    "            \"chunk_index\": int(r.chunk_index) if r.chunk_index is not None else None,\n",
    "            \"chunk_text\": r.chunk_text,\n",
    "            \"category\": r.category,\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "\n",
    "    scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return scored[:k]\n",
    "\n",
    "# ---------- Option B: Databricks Vector Search ----------\n",
    "\n",
    "def retrieve_top_k_optionB(query_embedding, k=5, filters=None):\n",
    "\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "    vsc = VectorSearchClient()\n",
    "\n",
    "    VS_ENDPOINT = \"vs_azure_compute\"\n",
    "    VS_INDEX_FULLNAME = f\"{CATALOG}.{SCHEMA}.azure_compute_docs_vs_index\"\n",
    "\n",
    "    index = vsc.get_index(endpoint_name=VS_ENDPOINT, index_name=VS_INDEX_FULLNAME)\n",
    "\n",
    "    vs_result = index.similarity_search(\n",
    "        query_vector=query_embedding,\n",
    "        columns=[\n",
    "            \"chunk_id\",\n",
    "            \"doc_id\",\n",
    "            \"title\",\n",
    "            \"url\",\n",
    "            \"chunk_index\",\n",
    "            \"chunk_text\",\n",
    "            \"category\",\n",
    "            # no store\n",
    "        ],\n",
    "        num_results=k,\n",
    "        filters=filters\n",
    "    )\n",
    "\n",
    "    rows = vs_result[\"result\"][\"data_array\"]\n",
    "\n",
    "    normalized = []\n",
    "    for r in rows:\n",
    "        normalized.append({\n",
    "            \"chunk_id\": r[0],\n",
    "            \"doc_id\": r[1],\n",
    "            \"title\": r[2],\n",
    "            \"url\": r[3],\n",
    "            \"chunk_index\": r[4],\n",
    "            \"chunk_text\": r[5],\n",
    "            \"category\": r[6],\n",
    "            \"score\": None   # Vector Search does not always return score\n",
    "        })\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# ---------- Wrapper: choose A or B, same output ----------\n",
    "# Both will return this structure:\n",
    "# {\n",
    "#   \"chunk_id\": ...,\n",
    "#   \"doc_id\": ...,\n",
    "#   \"title\": ...,\n",
    "#   \"url\": ...,\n",
    "#   \"chunk_index\": ...,\n",
    "#   \"chunk_text\": ...,\n",
    "#   \"category\": ...,\n",
    "#   \"score\": ...\n",
    "# }\n",
    "\n",
    "def retrieve_top_k(query_embedding, k=5, option=\"A\", **kwargs) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Unified retrieval entrypoint.\n",
    "    - option=\"A\": brute-force cosine similarity\n",
    "    - option=\"B\": vector search\n",
    "    Always returns list[dict] with the same schema.\n",
    "    \"\"\"\n",
    "    option = option.upper().strip()\n",
    "    if option == \"A\":\n",
    "        return retrieve_top_k_optionA(query_embedding, k=k, **kwargs)\n",
    "    elif option == \"B\":\n",
    "        return retrieve_top_k_optionB(query_embedding, k=k, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"option must be 'A' or 'B'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7327d107-64ae-4e15-abe7-f05ca6949756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_rag_event(event: dict):\n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import Row\n",
    "    # from pyspark.sql.types import *\n",
    "\n",
    "    rag_log_schema = StructType([\n",
    "        StructField(\"query_id\", StringType(), True),\n",
    "        StructField(\"question\", StringType(), True),\n",
    "        StructField(\"top_k\", IntegerType(), True),\n",
    "        StructField(\"retrieved_chunks\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"chunk_id\", StringType(), True),\n",
    "                StructField(\"doc_id\", StringType(), True),\n",
    "                StructField(\"title\", StringType(), True),\n",
    "                StructField(\"url\", StringType(), True),\n",
    "                StructField(\"chunk_index\", IntegerType(), True),\n",
    "                StructField(\"category\", StringType(), True),\n",
    "                StructField(\"score\", DoubleType(), True),\n",
    "            ])\n",
    "        ), True),\n",
    "        StructField(\"prompt\", StringType(), True),\n",
    "        StructField(\"answer\", StringType(), True),\n",
    "        StructField(\"embedding_deployment\", StringType(), True),\n",
    "        StructField(\"chat_deployment\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "    ])\n",
    "\n",
    "    query_id = str(uuid.uuid4())\n",
    "\n",
    "    # This produces a timezone-aware UTC timestamp.\n",
    "    now = datetime.now(UTC)\n",
    "\n",
    "    row_data = [{\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": str(event[\"question\"]),\n",
    "        \"top_k\": int(event[\"top_k\"]),\n",
    "        \"retrieved_chunks\": event[\"retrieved_chunks\"],\n",
    "        \"prompt\": str(event[\"prompt\"]),\n",
    "        \"answer\": str(event[\"answer\"]),\n",
    "        \"embedding_deployment\": str(event[\"embedding_deployment\"]),\n",
    "        \"chat_deployment\": str(event[\"chat_deployment\"]),\n",
    "        \"created_at\": now\n",
    "    }]\n",
    "\n",
    "    df = spark.createDataFrame(row_data, schema=rag_log_schema)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"databricks_rag_demo.default.rag_query_logs\")\n",
    "    )\n",
    "\n",
    "    return query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fac0367-6593-4b6f-a27f-fe20dac68be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prompt assembly\n",
    "\n",
    "def build_prompt(question, contexts):\n",
    "    joined_context = \"\\n\\n\".join(contexts)\n",
    "    return f\"\"\"\n",
    "You are a helpful assistant answering questions about Azure Compute.\n",
    "\n",
    "Use the following documentation excerpts to answer the question.\n",
    "\n",
    "Context:\n",
    "{joined_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed282a49-9391-4509-bfae-6caaee185d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### LLM judge\n",
    "\n",
    "###### Judge prompt builder\n",
    "import json\n",
    "\n",
    "def build_judge_prompt(question: str, answer: str, retrieved_chunks: list[dict]) -> str:\n",
    "    # keep judge prompt compact: only short excerpts\n",
    "    excerpts = []\n",
    "    for i, c in enumerate(retrieved_chunks[:6], start=1):\n",
    "        chunk_text = (c.get(\"chunk_text\") or \"\")[:1200]  # cap to avoid giant prompts\n",
    "        url = c.get(\"url\")\n",
    "        excerpts.append(f\"[{i}] URL: {url}\\nEXCERPT:\\n{chunk_text}\")\n",
    "\n",
    "    sources_block = \"\\n\\n---\\n\\n\".join(excerpts)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are evaluating a Retrieval-Augmented Generation (RAG) system for Azure Compute documentation.\n",
    "\n",
    "Score the system on a 1 - 5 scale (integers only):\n",
    "- retrieval_relevance: are the retrieved excerpts relevant to the question?\n",
    "- answer_relevance: does the answer address the question?\n",
    "- faithfulness: is the answer supported by the provided excerpts (no hallucination)?\n",
    "\n",
    "Return ONLY valid JSON with keys:\n",
    "retrieval_relevance, answer_relevance, faithfulness, notes\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Retrieved sources:\n",
    "{sources_block}\n",
    "\"\"\"\n",
    "\n",
    "###### Judge function (Azure OpenAI chat)\n",
    "\n",
    "def judge_rag(question: str, answer: str, retrieved_chunks: list[dict]) -> dict:\n",
    "    prompt = build_judge_prompt(question, answer, retrieved_chunks)\n",
    "\n",
    "    resp = aoai.chat.completions.create(\n",
    "        model=CHAT_DEPLOYMENT,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # parse JSON robustly\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: try to extract JSON substring if model added extra text\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        data = json.loads(text[start:end+1])\n",
    "\n",
    "    # enforce integers 1..5\n",
    "    def clamp_int(x):\n",
    "        x = int(x)\n",
    "        return max(1, min(5, x))\n",
    "\n",
    "    return {\n",
    "        \"retrieval_relevance\": clamp_int(data.get(\"retrieval_relevance\", 3)),\n",
    "        \"answer_relevance\": clamp_int(data.get(\"answer_relevance\", 3)),\n",
    "        \"faithfulness\": clamp_int(data.get(\"faithfulness\", 3)),\n",
    "        \"notes\": str(data.get(\"notes\", \"\")).strip()[:2000]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf613c9-7d30-4ed1-9387-c6bf2e26a0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Write evaluation results to Delta\n",
    "\n",
    "import uuid\n",
    "from datetime import datetime, UTC\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def write_evaluation(query_id: str, question: str, answer: str, scores: dict, evaluator=\"llm_judge_v1\"):\n",
    "\n",
    "    evaluation_id = str(uuid.uuid4())\n",
    "    now = datetime.now(UTC)\n",
    "\n",
    "    rag_eval_schema = StructType([\n",
    "        StructField(\"evaluation_id\", StringType(), True),\n",
    "        StructField(\"query_id\", StringType(), True),\n",
    "        StructField(\"question\", StringType(), True),\n",
    "        StructField(\"answer\", StringType(), True),\n",
    "        StructField(\"retrieval_relevance\", IntegerType(), True),\n",
    "        StructField(\"answer_relevance\", IntegerType(), True),\n",
    "        StructField(\"faithfulness\", IntegerType(), True),\n",
    "        StructField(\"evaluator\", StringType(), True),\n",
    "        StructField(\"notes\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "    ])\n",
    "\n",
    "    row = Row(\n",
    "        evaluation_id=evaluation_id,\n",
    "        query_id=query_id,\n",
    "        question=str(question),\n",
    "        answer=str(answer),\n",
    "        retrieval_relevance=int(scores[\"retrieval_relevance\"]),  # <-- FORCE\n",
    "        answer_relevance=int(scores[\"answer_relevance\"]),        # <-- FORCE\n",
    "        faithfulness=int(scores[\"faithfulness\"]),                # <-- FORCE\n",
    "        evaluator=str(evaluator),\n",
    "        notes=str(scores.get(\"notes\", \"\")),\n",
    "        created_at=now\n",
    "    )\n",
    "\n",
    "    (\n",
    "        spark.createDataFrame([row], schema=rag_eval_schema)\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"databricks_rag_demo.default.rag_evaluations\")\n",
    "    )\n",
    "\n",
    "    return evaluation_id"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}