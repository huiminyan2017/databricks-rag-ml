{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5392495-8dbf-4ac3-8a14-0d8cb9729b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07faa7d7-c26b-402e-8a53-05a20e4a661d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The * means: All parameters after * must be passed as keyword arguments.\n",
    "def embed_texts(texts, *, single: bool = None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a single string or a list of strings.\n",
    "\n",
    "    Args:\n",
    "        texts: str or list[str]\n",
    "        single: Optional[bool]. If True, returns a single embedding.\n",
    "                If False, returns a list of embeddings.\n",
    "                If None, inferred from input type.\n",
    "\n",
    "    Returns:\n",
    "        If single=True or input is str -> list[float]\n",
    "        If single=False or input is list[str] -> list[list[float]]\n",
    "    \"\"\"\n",
    "    # Normalize input\n",
    "    if isinstance(texts, str):\n",
    "        inputs = [texts]\n",
    "        inferred_single = True\n",
    "    elif isinstance(texts, list):\n",
    "        inputs = texts\n",
    "        inferred_single = False\n",
    "    else:\n",
    "        raise TypeError(\"texts must be a string or list of strings\")\n",
    "\n",
    "    if single is None:\n",
    "        single = inferred_single\n",
    "\n",
    "    response = aoai.embeddings.create(\n",
    "        model=EMBEDDING_DEPLOYMENT,\n",
    "        input=inputs\n",
    "    )\n",
    "\n",
    "    embeddings = [d.embedding for d in response.data]\n",
    "\n",
    "    if single:\n",
    "        return embeddings[0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d049aa0b-158c-4d05-b3cc-c19166aee84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cosine similarity function\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75066a79-c14e-4a98-9cc9-375eb91ad1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Option A: brute-force cosine similarity ----------\n",
    "\n",
    "def retrieve_top_k_optionA(query_embedding, k=5, limit=2000):\n",
    "\n",
    "    # Load all embedding rows\n",
    "    embeddings_df = spark.table(EMB_TABLE)\n",
    "\n",
    "    rows = embeddings_df.select(\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"chunk_text\",\n",
    "        \"category\",\n",
    "        \"embedding\"\n",
    "    ).limit(limit).collect()\n",
    "\n",
    "    scored = []\n",
    "\n",
    "    for r in rows:\n",
    "        score = cosine_similarity(query_embedding, r.embedding)\n",
    "        scored.append({\n",
    "            \"chunk_id\": r.chunk_id,\n",
    "            \"doc_id\": r.doc_id,\n",
    "            \"title\": r.title,\n",
    "            \"url\": r.url,\n",
    "            \"chunk_index\": int(r.chunk_index) if r.chunk_index is not None else None,\n",
    "            \"chunk_text\": r.chunk_text,\n",
    "            \"category\": r.category,\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "\n",
    "    scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return scored[:k]\n",
    "\n",
    "# ---------- Option B: Databricks Vector Search ----------\n",
    "\n",
    "def retrieve_top_k_optionB(query_embedding, k=5, filters=None):\n",
    "\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "    vsc = VectorSearchClient()\n",
    "\n",
    "    VS_ENDPOINT = \"vs_azure_compute\"\n",
    "    VS_INDEX_FULLNAME = f\"{CATALOG}.{SCHEMA}.azure_compute_docs_vs_index\"\n",
    "\n",
    "    index = vsc.get_index(endpoint_name=VS_ENDPOINT, index_name=VS_INDEX_FULLNAME)\n",
    "\n",
    "    vs_result = index.similarity_search(\n",
    "        query_vector=query_embedding,\n",
    "        columns=[\n",
    "            \"chunk_id\",\n",
    "            \"doc_id\",\n",
    "            \"title\",\n",
    "            \"url\",\n",
    "            \"chunk_index\",\n",
    "            \"chunk_text\",\n",
    "            \"category\",\n",
    "            # no store\n",
    "        ],\n",
    "        num_results=k,\n",
    "        filters=filters\n",
    "    )\n",
    "\n",
    "    rows = vs_result[\"result\"][\"data_array\"]\n",
    "\n",
    "    normalized = []\n",
    "    for r in rows:\n",
    "        normalized.append({\n",
    "            \"chunk_id\": r[0],\n",
    "            \"doc_id\": r[1],\n",
    "            \"title\": r[2],\n",
    "            \"url\": r[3],\n",
    "            \"chunk_index\": r[4],\n",
    "            \"chunk_text\": r[5],\n",
    "            \"category\": r[6],\n",
    "            \"score\": None   # Vector Search does not always return score\n",
    "        })\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# ---------- Wrapper: choose A or B, same output ----------\n",
    "# Both will return this structure:\n",
    "# {\n",
    "#   \"chunk_id\": ...,\n",
    "#   \"doc_id\": ...,\n",
    "#   \"title\": ...,\n",
    "#   \"url\": ...,\n",
    "#   \"chunk_index\": ...,\n",
    "#   \"chunk_text\": ...,\n",
    "#   \"category\": ...,\n",
    "#   \"score\": ...\n",
    "# }\n",
    "\n",
    "def retrieve_top_k(query_embedding, k=5, option=\"A\", **kwargs) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Unified retrieval entrypoint.\n",
    "    - option=\"A\": brute-force cosine similarity\n",
    "    - option=\"B\": vector search\n",
    "    Always returns list[dict] with the same schema.\n",
    "    \"\"\"\n",
    "    option = option.upper().strip()\n",
    "    if option == \"A\":\n",
    "        return retrieve_top_k_optionA(query_embedding, k=k, **kwargs)\n",
    "    elif option == \"B\":\n",
    "        return retrieve_top_k_optionB(query_embedding, k=k, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"option must be 'A' or 'B'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7327d107-64ae-4e15-abe7-f05ca6949756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_rag_log_table(spark, table_name: str):\n",
    "    \"\"\"\n",
    "    Ensure the RAG query log Delta table exists with the correct schema.\n",
    "    Safe to call multiple times (idempotent).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "    table_name : str\n",
    "        Fully qualified table name, e.g. \"catalog.schema.rag_query_logs\"\n",
    "    \"\"\"\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "      query_id STRING,\n",
    "      question STRING,\n",
    "      top_k INT,\n",
    "      retriever_type STRING,\n",
    "\n",
    "      retrieved_chunks ARRAY<STRUCT<\n",
    "        chunk_id: STRING,\n",
    "        doc_id: STRING,\n",
    "        title: STRING,\n",
    "        url: STRING,\n",
    "        chunk_index: INT,\n",
    "        category: STRING,\n",
    "        score: DOUBLE\n",
    "      >>,\n",
    "\n",
    "      prompt STRING,\n",
    "      answer STRING,\n",
    "      embedding_deployment STRING,\n",
    "      chat_deployment STRING,\n",
    "      created_at TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(ddl)\n",
    "    print(f\"✅ RAG log table ensured: {table_name}\")\n",
    "\n",
    "def log_rag_event(event: dict):\n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import Row\n",
    "    # from pyspark.sql.types import *\n",
    "\n",
    "    rag_log_schema = StructType([\n",
    "        StructField(\"query_id\", StringType(), True),\n",
    "        StructField(\"question\", StringType(), True),\n",
    "        StructField(\"top_k\", IntegerType(), True),\n",
    "        StructField(\"retriever_type\", StringType(), True),\n",
    "        StructField(\"retrieved_chunks\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"chunk_id\", StringType(), True),\n",
    "                StructField(\"doc_id\", StringType(), True),\n",
    "                StructField(\"title\", StringType(), True),\n",
    "                StructField(\"url\", StringType(), True),\n",
    "                StructField(\"chunk_index\", IntegerType(), True),\n",
    "                StructField(\"category\", StringType(), True),\n",
    "                StructField(\"score\", DoubleType(), True),\n",
    "            ])\n",
    "        ), True),\n",
    "        StructField(\"prompt\", StringType(), True),\n",
    "        StructField(\"answer\", StringType(), True),\n",
    "        StructField(\"embedding_deployment\", StringType(), True),\n",
    "        StructField(\"chat_deployment\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "    ])\n",
    "\n",
    "    query_id = str(uuid.uuid4())\n",
    "\n",
    "    # This produces a timezone-aware UTC timestamp.\n",
    "    now = datetime.now(UTC)\n",
    "\n",
    "    row_data = [{\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": str(event[\"question\"]),\n",
    "        \"top_k\": int(event[\"top_k\"]),\n",
    "        \"retriever_type\": event[\"retriever_type\"],\n",
    "        \"retrieved_chunks\": event[\"retrieved_chunks\"],\n",
    "        \"prompt\": str(event[\"prompt\"]),\n",
    "        \"answer\": str(event[\"answer\"]),\n",
    "        \"embedding_deployment\": str(event[\"embedding_deployment\"]),\n",
    "        \"chat_deployment\": str(event[\"chat_deployment\"]),\n",
    "        \"created_at\": now\n",
    "    }]\n",
    "\n",
    "    df = spark.createDataFrame(row_data, schema=rag_log_schema)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"databricks_rag_demo.default.rag_query_logs\")\n",
    "    )\n",
    "\n",
    "    return query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fac0367-6593-4b6f-a27f-fe20dac68be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prompt assembly\n",
    "\n",
    "def build_prompt(question, contexts):\n",
    "    joined_context = \"\\n\\n\".join(contexts)\n",
    "    return f\"\"\"\n",
    "You are a helpful assistant answering questions about Azure Compute.\n",
    "\n",
    "Use the following documentation excerpts to answer the question.\n",
    "\n",
    "Context:\n",
    "{joined_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed282a49-9391-4509-bfae-6caaee185d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### LLM judge\n",
    "\n",
    "###### Judge prompt builder\n",
    "import json\n",
    "\n",
    "def build_judge_prompt(question: str, answer: str, retrieved_chunks: list[dict]) -> str:\n",
    "    # keep judge prompt compact: only short excerpts\n",
    "    excerpts = []\n",
    "    for i, c in enumerate(retrieved_chunks[:6], start=1):\n",
    "        chunk_text = (c.get(\"chunk_text\") or \"\")[:1200]  # cap to avoid giant prompts\n",
    "        url = c.get(\"url\")\n",
    "        excerpts.append(f\"[{i}] URL: {url}\\nEXCERPT:\\n{chunk_text}\")\n",
    "\n",
    "    sources_block = \"\\n\\n---\\n\\n\".join(excerpts)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are evaluating a Retrieval-Augmented Generation (RAG) system for Azure Compute documentation.\n",
    "\n",
    "Score the system on a 1 - 5 scale (integers only):\n",
    "- retrieval_relevance: are the retrieved excerpts relevant to the question?\n",
    "- answer_relevance: does the answer address the question?\n",
    "- faithfulness: is the answer supported by the provided excerpts (no hallucination)?\n",
    "\n",
    "Return ONLY valid JSON with keys:\n",
    "retrieval_relevance, answer_relevance, faithfulness, notes\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Retrieved sources:\n",
    "{sources_block}\n",
    "\"\"\"\n",
    "\n",
    "###### Judge function (Azure OpenAI chat)\n",
    "\n",
    "def judge_rag(question: str, answer: str, retrieved_chunks: list[dict]) -> dict:\n",
    "    prompt = build_judge_prompt(question, answer, retrieved_chunks)\n",
    "\n",
    "    resp = aoai.chat.completions.create(\n",
    "        model=CHAT_DEPLOYMENT,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # parse JSON robustly\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: try to extract JSON substring if model added extra text\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        data = json.loads(text[start:end+1])\n",
    "\n",
    "    # enforce integers 1..5\n",
    "    def clamp_int(x):\n",
    "        x = int(x)\n",
    "        return max(1, min(5, x))\n",
    "\n",
    "    return {\n",
    "        \"retrieval_relevance\": clamp_int(data.get(\"retrieval_relevance\", 3)),\n",
    "        \"answer_relevance\": clamp_int(data.get(\"answer_relevance\", 3)),\n",
    "        \"faithfulness\": clamp_int(data.get(\"faithfulness\", 3)),\n",
    "        \"notes\": str(data.get(\"notes\", \"\")).strip()[:2000]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf613c9-7d30-4ed1-9387-c6bf2e26a0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "from datetime import datetime, UTC\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def ensure_rag_eval_table(spark, table_name: str):\n",
    "    \"\"\"\n",
    "    Ensure the RAG evaluation Delta table exists with the correct schema.\n",
    "    Safe to call multiple times (idempotent).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "    table_name : str\n",
    "        Fully qualified table name, e.g. \"catalog.schema.rag_evaluations\"\n",
    "    \"\"\"\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "      evaluation_id STRING,\n",
    "      query_id STRING,\n",
    "      question STRING,\n",
    "      answer STRING,\n",
    "      retrieval_relevance INT,\n",
    "      answer_relevance INT,\n",
    "      faithfulness INT,\n",
    "      evaluator STRING,\n",
    "      notes STRING,\n",
    "      created_at TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(ddl)\n",
    "    print(f\"✅ RAG evaluation table ensured: {table_name}\")\n",
    "\n",
    "### Write evaluation results to Delta\n",
    "def write_evaluation(query_id: str, question: str, answer: str, scores: dict, evaluator=\"llm_judge_v1\"):\n",
    "\n",
    "    evaluation_id = str(uuid.uuid4())\n",
    "    now = datetime.now(UTC)\n",
    "\n",
    "    rag_eval_schema = StructType([\n",
    "        StructField(\"evaluation_id\", StringType(), True),\n",
    "        StructField(\"query_id\", StringType(), True),\n",
    "        StructField(\"question\", StringType(), True),\n",
    "        StructField(\"answer\", StringType(), True),\n",
    "        StructField(\"retrieval_relevance\", IntegerType(), True),\n",
    "        StructField(\"answer_relevance\", IntegerType(), True),\n",
    "        StructField(\"faithfulness\", IntegerType(), True),\n",
    "        StructField(\"evaluator\", StringType(), True),\n",
    "        StructField(\"notes\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "    ])\n",
    "\n",
    "    row = Row(\n",
    "        evaluation_id=evaluation_id,\n",
    "        query_id=query_id,\n",
    "        question=str(question),\n",
    "        answer=str(answer),\n",
    "        retrieval_relevance=int(scores[\"retrieval_relevance\"]),  # <-- FORCE\n",
    "        answer_relevance=int(scores[\"answer_relevance\"]),        # <-- FORCE\n",
    "        faithfulness=int(scores[\"faithfulness\"]),                # <-- FORCE\n",
    "        evaluator=str(evaluator),\n",
    "        notes=str(scores.get(\"notes\", \"\")),\n",
    "        created_at=now\n",
    "    )\n",
    "    (\n",
    "        spark.createDataFrame([row], schema=rag_eval_schema)\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(RAG_EVAL_TABLE)\n",
    "    )\n",
    "    return evaluation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c7ec56-6ac2-433d-9c1a-9990e7c8b2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Enrich chunks with chunk_text\n",
    "def enrich_chunks_with_text(retrieved_chunks_py: list[dict], chunks_df) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Given a list of chunk dicts (with chunk_id), attach chunk_text from chunks_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    retrieved_chunks_py : list[dict]\n",
    "        Each dict must contain \"chunk_id\"\n",
    "    chunks_df : Spark DataFrame\n",
    "        Must contain columns: chunk_id, chunk_text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict]\n",
    "        Same list, enriched with \"chunk_text\"\n",
    "    \"\"\"\n",
    "    chunk_ids = [c[\"chunk_id\"] for c in retrieved_chunks_py if c.get(\"chunk_id\")]\n",
    "    if not chunk_ids:\n",
    "        return retrieved_chunks_py\n",
    "\n",
    "    rows = (\n",
    "        chunks_df\n",
    "        .where(F.col(\"chunk_id\").isin(chunk_ids))\n",
    "        .select(\"chunk_id\", \"chunk_text\")\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    text_map = {r[\"chunk_id\"]: r[\"chunk_text\"] for r in rows}\n",
    "\n",
    "    for c in retrieved_chunks_py:\n",
    "        cid = c.get(\"chunk_id\")\n",
    "        c[\"chunk_text\"] = text_map.get(cid, \"\")\n",
    "\n",
    "    return retrieved_chunks_py\n",
    "\n",
    "# Convert logged row → Python dict chunks, Your log table stores retrieved_chunks as Spark structs. This helper converts them to normal Python dicts.\n",
    "def normalize_logged_chunks(log_row) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Convert Spark struct array -> list of Python dicts.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for c in log_row[\"retrieved_chunks\"]:\n",
    "        out.append({\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"doc_id\": c[\"doc_id\"],\n",
    "            \"title\": c[\"title\"],\n",
    "            \"url\": c[\"url\"],\n",
    "            \"chunk_index\": c[\"chunk_index\"],\n",
    "            \"category\": c[\"category\"],\n",
    "            \"score\": c[\"score\"],\n",
    "            \"chunk_text\": \"\"  # will be enriched later\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# Run LLM evaluation on recent logs\n",
    "def evaluate_recent_logs(\n",
    "    spark,\n",
    "    rag_log_table: str,\n",
    "    chunks_table: str,\n",
    "    n: int,\n",
    "    judge_fn,\n",
    "    write_eval_fn,\n",
    "    evaluator_name=\"llm_judge_with_text\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run evaluation on the most recent N RAG queries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "    rag_log_table : str\n",
    "        Fully qualified table name\n",
    "    chunks_table : str\n",
    "        Fully qualified table name\n",
    "    n : int\n",
    "        Number of recent logs to evaluate\n",
    "    judge_fn : function\n",
    "        judge_rag(question, answer, retrieved_chunks) -> scores\n",
    "    write_eval_fn : function\n",
    "        write_evaluation(query_id, question, answer, scores, evaluator=...)\n",
    "    evaluator_name : str\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    logs_df = spark.table(rag_log_table)\n",
    "    chunks_df = spark.table(chunks_table).select(\"chunk_id\", \"chunk_text\")\n",
    "\n",
    "    recent_logs = (\n",
    "        logs_df\n",
    "        .orderBy(F.col(\"created_at\").desc())\n",
    "        .limit(n)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    for r in recent_logs:\n",
    "        query_id = r[\"query_id\"]\n",
    "        question = r[\"question\"]\n",
    "        answer = r[\"answer\"]\n",
    "\n",
    "        retrieved_chunks_py = normalize_logged_chunks(r)\n",
    "        retrieved_chunks_py = enrich_chunks_with_text(retrieved_chunks_py, chunks_df)\n",
    "\n",
    "        scores = judge_fn(question, answer, retrieved_chunks_py)\n",
    "        eval_id = write_eval_fn(\n",
    "            query_id=query_id,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            scores=scores,\n",
    "            evaluator=evaluator_name\n",
    "        )\n",
    "\n",
    "        print(\"✅ Evaluated:\", query_id, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631acdaf-8547-42c7-ba19-7f3de6090688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The unified pipeline function: ask()\n",
    "# embed → retrieve → prompt → Azure OpenAI → log → (optional) evaluate → return JSON.\n",
    "\n",
    "def ask(\n",
    "    question: str,\n",
    "    *,\n",
    "    k: int = 6,\n",
    "    retriever: str = \"A\",     # \"A\" brute-force, \"B\" vector search\n",
    "    do_eval: bool = True,     # run LLM judge + write to rag_evaluations\n",
    "    filters: dict = None,     # optional metadata filters for Vector Search\n",
    "    temperature: float = 0.2,\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    End-to-end RAG call:\n",
    "      - Embed\n",
    "      - Retrieve\n",
    "      - Prompt\n",
    "      - Azure OpenAI\n",
    "      - Log\n",
    "      - Optional evaluation\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        query_id: str,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        sources: [...],\n",
    "        eval: {...} | None\n",
    "      }\n",
    "    \"\"\"\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        raise ValueError(\"question must be a non-empty string\")\n",
    "\n",
    "    retriever = retriever.upper().strip()\n",
    "    if retriever not in (\"A\", \"B\"):\n",
    "        raise ValueError(\"retriever must be 'A' or 'B'\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n\uD83D\uDD39 Question: {question}\")\n",
    "        print(f\"\uD83D\uDD39 Retriever: {retriever}\")\n",
    "\n",
    "    # 1) Embed\n",
    "    q_emb = embed_texts(question)\n",
    "\n",
    "    # 2) Retrieve\n",
    "    if retriever == \"B\":\n",
    "        chunks = retrieve_top_k(q_emb, option=\"B\", k=k, filters=filters)\n",
    "    else:\n",
    "        chunks = retrieve_top_k(q_emb, option=\"A\", k=k)\n",
    "\n",
    "    _validate_chunks(chunks)\n",
    "\n",
    "    # 3) Prompt\n",
    "    chunks_text = [c[\"chunk_text\"] for c in chunks]\n",
    "    prompt = build_prompt(question, chunks_text)\n",
    "\n",
    "    # 4) Call LLM\n",
    "    resp = aoai.chat.completions.create(\n",
    "        model=CHAT_DEPLOYMENT,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    answer = resp.choices[0].message.content\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n",
    "\n",
    "    # 5) Log\n",
    "    rag_event = {\n",
    "        \"question\": question,\n",
    "        \"top_k\": k,\n",
    "        \"retriever_type\": retriever,\n",
    "        \"retrieved_chunks\": chunks,\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": answer,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"chat_deployment\": CHAT_DEPLOYMENT\n",
    "    }\n",
    "    query_id = log_rag_event(rag_event)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Logged query_id:\", query_id)\n",
    "\n",
    "    # 6) Optional evaluation\n",
    "    eval_result = None\n",
    "    if do_eval:\n",
    "        eval_scores = judge_rag(question, answer, chunks)\n",
    "        eval_id = write_evaluation(\n",
    "            query_id=query_id,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            scores=eval_scores,\n",
    "            evaluator=\"llm_judge_with_text\"\n",
    "        )\n",
    "        eval_result = {\n",
    "            \"evaluation_id\": eval_id,\n",
    "            **eval_scores\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Eval:\", eval_result)\n",
    "\n",
    "    return {\n",
    "        \"query_id\": query_id,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": _compact_sources(chunks),\n",
    "        \"eval\": eval_result\n",
    "    }\n",
    "\n",
    "def batch_ask(\n",
    "    questions: list[str],\n",
    "    *,\n",
    "    k: int = 6,\n",
    "    retrievers=[\"A\", \"B\"],\n",
    "    do_eval: bool = True,\n",
    "    filters: dict = None,\n",
    "    temperature: float = 0.2,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run multiple questions across multiple retrievers.\n",
    "\n",
    "    Returns:\n",
    "      list of results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for q in questions:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Question:\", q)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for r in retrievers:\n",
    "            print(f\"\\n--- Retriever {r} ---\")\n",
    "\n",
    "            res = ask(\n",
    "                q,\n",
    "                k=k,\n",
    "                retriever=r,\n",
    "                do_eval=do_eval,\n",
    "                filters=filters,\n",
    "                temperature=temperature,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            results.append(res)\n",
    "\n",
    "    return results\n",
    "\n",
    "def _validate_chunks(chunks: list[dict]):\n",
    "    if not isinstance(chunks, list):\n",
    "        raise TypeError(\"retrieve_top_k must return a list[dict]\")\n",
    "\n",
    "    if len(chunks) == 0:\n",
    "        raise ValueError(\"No chunks were retrieved\")\n",
    "\n",
    "    required_keys = {\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"category\",\n",
    "        \"score\",\n",
    "        \"chunk_text\",\n",
    "    }\n",
    "\n",
    "    for i, c in enumerate(chunks):\n",
    "        if not isinstance(c, dict):\n",
    "            raise TypeError(f\"Chunk at index {i} is not a dict\")\n",
    "\n",
    "        missing = required_keys - set(c.keys())\n",
    "        if missing:\n",
    "            raise ValueError(f\"Chunk at index {i} is missing keys: {missing}\")\n",
    "\n",
    "def _compact_sources(chunks: list[dict], max_chars: int = 200):\n",
    "    \"\"\"\n",
    "    Compact retrieved chunks for UI / API output.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "\n",
    "    for c in chunks:\n",
    "        text = c.get(\"chunk_text\", \"\") or \"\"\n",
    "        preview = text[:max_chars] + (\"…\" if len(text) > max_chars else \"\")\n",
    "\n",
    "        sources.append({\n",
    "            \"chunk_id\": c.get(\"chunk_id\"),\n",
    "            \"doc_id\": c.get(\"doc_id\"),\n",
    "            \"title\": c.get(\"title\"),\n",
    "            \"url\": c.get(\"url\"),\n",
    "            \"chunk_index\": c.get(\"chunk_index\"),\n",
    "            \"category\": c.get(\"category\"),\n",
    "            \"score\": c.get(\"score\"),\n",
    "            \"preview\": preview,\n",
    "        })\n",
    "\n",
    "    return sources"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}