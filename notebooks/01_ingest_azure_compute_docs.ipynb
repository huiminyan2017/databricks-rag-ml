{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34e5744e-d1bb-4afa-b4d7-9ea65f89e7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 01 - Ingest Azure Compute Documentation into databricks Unity Catalog\n",
    "\n",
    "This notebook downloads Azure Compute documentation from GitHub, cleans Markdown content, and writes the data into a Unity Catalog–managed Delta table.\n",
    "\n",
    "**Execution environment**\n",
    "- Run this notebook on **Azure Databricks (Premium tier)** with **Unity Catalog enabled**\n",
    "- Uses a **single-user Databricks cluster** (DBR 15+)\n",
    "- Writes data as **Unity Catalog–managed Delta tables**\n",
    "\n",
    "**What this notebook does**\n",
    "- Downloads the latest Azure Compute documentation from the public GitHub repository  \n",
    "  `MicrosoftDocs/azure-compute-docs` using a shallow Git clone\n",
    "- Parses and cleans Markdown files under the `articles/` directory\n",
    "- Extracts metadata such as document ID, category, title, source URL, and ingestion time\n",
    "- Persists the processed documents into a governed Delta table:\n",
    "databricks_rag_demo.default.raw_azure_compute_docs\n",
    "\n",
    "This notebook establishes the **raw document ingestion layer** for a\n",
    "Retrieval-Augmented Generation (RAG) pipeline and intentionally avoids\n",
    "legacy DBFS-based storage in favor of **Unity Catalog–managed data objects**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1186e6d6-b5e8-46b8-9f34-c3170bc5fb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog</th></tr></thead><tbody><tr><td>databricks_rag_demo</td></tr><tr><td>samples</td></tr><tr><td>system</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "databricks_rag_demo"
        ],
        [
         "samples"
        ],
        [
         "system"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "----- workspace will create a default catelog with same name as the workspace, we will mostly work in this catelog\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5726b423-052c-4cf2-8157-5314e04ccb06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>default</td></tr><tr><td>information_schema</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default"
        ],
        [
         "information_schema"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "databaseName",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql SHOW SCHEMAS IN databricks_rag_demo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15ad8c0-5230-4a9f-83fd-49f1ace94ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython in /databricks/python3/lib/python3.12/site-packages (3.1.37)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython) (4.0.11)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "##### gitpython is a Python wrapper around the git command. We will use this to do git clone\n",
    "\n",
    "%pip install gitpython # gitpython is a Python wrapper around the git command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e9b03a-6e98-41f2-995b-b98eb1632eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from git import Repo\n",
    "import shutil\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67174a16-b22f-4850-a839-eaa244543f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/MicrosoftDocs/azure-compute-docs.git\"\n",
    "\n",
    "TARGET_DIR = \"/tmp/azure-compute-docs\" # it will be created on the driver VM’s local disk.\n",
    "TARGET_ARTICLE_PATH = f\"{TARGET_DIR}/articles\"\n",
    "\n",
    "DEFAULT_CATELOG_NAME = \"databricks_rag_demo\"\n",
    "TABLE_NAME=\"raw_azure_compute_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e37220a-f89d-48e2-bfc7-a2d24f30fd7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh rm -rf /tmp/azure-compute-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c159fe-13cf-4a9b-b1c4-4cd7ad3cb0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['service-fabric',\n",
       " 'virtual-machines',\n",
       " 'container-instances',\n",
       " 'azure-impact-reporting',\n",
       " 'virtual-machine-scale-sets']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_azure_compute_docs():\n",
    "    \n",
    "    # SHALLOW clone\n",
    "    Repo.clone_from(\n",
    "        REPO_URL,\n",
    "        TARGET_DIR,\n",
    "        depth=1 # Depth = how much git history you download\n",
    "    )\n",
    "\n",
    "download_azure_compute_docs()\n",
    "os.listdir(f\"{TARGET_ARTICLE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d72f46-fd9d-44ef-9a0d-fa00d2f23a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean markdown text\n",
    "def clean_markdown(md_text: str) -> str:\n",
    "    # Remove code blocks\n",
    "    #md_text = re.sub(r\"```.*?```\", \"\", md_text, flags=re.S)\n",
    "    # Remove images\n",
    "    md_text = re.sub(r\"!\\[.*?\\]\\(.*?\\)\", \"\", md_text)\n",
    "    # Remove links but keep text\n",
    "    md_text = re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", md_text)\n",
    "    # Remove headings symbols\n",
    "    md_text = re.sub(r\"#+ \", \"\", md_text)\n",
    "    return md_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b2542d-4569-4243-ac92-0ef834f2af23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-c6c441ab-7e09-494b-bf0b-82/.ipykernel/1886/command-8004489376597638-4245322713:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ingest_time=datetime.utcnow()\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>doc_id</th><th>source</th><th>category</th><th>title</th><th>raw_text</th><th>url</th><th>ingest_time</th></tr></thead><tbody><tr><td>service-fabric/service-fabric-best-practices-infrastructure-as-code.md</td><td>azure-compute-docs</td><td>service-fabric</td><td>service-fabric-best-practices-infrastructure-as-code</td><td>---\n",
       "title: Azure Service Fabric infrastructure as Code Best Practices\n",
       "description: Best practices and design considerations for managing Azure Service Fabric as infrastructure as code.\n",
       "ms.topic: concept-article\n",
       "ms.author: tomcassidy\n",
       "author: tomvcassidy\n",
       "ms.service: azure-service-fabric\n",
       "services: service-fabric\n",
       "ms.date: 07/14/2022\n",
       "Customer intent: \"As a cloud administrator, I want to utilize Infrastructure as Code to deploy and manage Azure Service Fabric clusters, so that I can ensure consistent and efficient resource configuration and maintenance.\"\n",
       "---\n",
       "\n",
       "Infrastructure as code\n",
       "\n",
       "In a production scenario, create Azure Service Fabric clusters using Resource Manager templates. Resource Manager templates provide greater control of resource properties and ensure that you have a consistent resource model.\n",
       "\n",
       "Sample Resource Manager templates are available for Windows and Linux in the Azure samples on GitHub. These templates can be used as a starting point for your cluster template. Download `azuredeploy.json` and `azuredeploy.parameters.json` and edit them to meet your custom requirements.\n",
       "\n",
       "!INCLUDE [updated-for-az]\n",
       "\n",
       "To deploy the `azuredeploy.json` and `azuredeploy.parameters.json` templates you downloaded above, use the following Azure CLI commands:\n",
       "\n",
       "```azurecli\n",
       "ResourceGroupName=\"sfclustergroup\"\n",
       "Location=\"westus\"\n",
       "\n",
       "az group create --name $ResourceGroupName --location $Location \n",
       "az deployment group create --name $ResourceGroupName  --template-file azuredeploy.json --parameters @azuredeploy.parameters.json\n",
       "```\n",
       "\n",
       "Creating a resource using PowerShell\n",
       "\n",
       "```powershell\n",
       "$ResourceGroupName=\"sfclustergroup\"\n",
       "$Location=\"westus\"\n",
       "$Template=\"azuredeploy.json\"\n",
       "$Parameters=\"azuredeploy.parameters.json\"\n",
       "\n",
       "New-AzResourceGroup -Name $ResourceGroupName -Location $Location\n",
       "New-AzResourceGroupDeployment -Name $ResourceGroupName -TemplateFile $Template -TemplateParameterFile $Parameters\n",
       "```\n",
       "\n",
       "Service Fabric resources\n",
       "\n",
       "You can deploy applications and services onto your Service Fabric cluster via Azure Resource Manager. See Manage applications and services as Azure Resource Manager resources for details. The following are best practice Service Fabric application specific resources to include in your  Resource Manager template resources.\n",
       "\n",
       "```json\n",
       "{\n",
       "    \"apiVersion\": \"2019-03-01\",\n",
       "    \"type\": \"Microsoft.ServiceFabric/clusters/applicationTypes\",\n",
       "    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationTypeName'))]\",\n",
       "    \"location\": \"[variables('clusterLocation')]\",\n",
       "},\n",
       "{\n",
       "    \"apiVersion\": \"2019-03-01\",\n",
       "    \"type\": \"Microsoft.ServiceFabric/clusters/applicationTypes/versions\",\n",
       "    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationTypeName'), '/', parameters('applicationTypeVersion'))]\",\n",
       "    \"location\": \"[variables('clusterLocation')]\",\n",
       "},\n",
       "{\n",
       "    \"apiVersion\": \"2019-03-01\",\n",
       "    \"type\": \"Microsoft.ServiceFabric/clusters/applications\",\n",
       "    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationName'))]\",\n",
       "    \"location\": \"[variables('clusterLocation')]\",\n",
       "},\n",
       "{\n",
       "    \"apiVersion\": \"2019-03-01\",\n",
       "    \"type\": \"Microsoft.ServiceFabric/clusters/applications/services\",\n",
       "    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationName'), '/', parameters('serviceName'))]\",\n",
       "    \"location\": \"[variables('clusterLocation')]\"\n",
       "}\n",
       "```\n",
       "\n",
       "To deploy your application using Azure Resource Manager, you first must create a sfpkg Service Fabric Application package. The following Python script is an example of how to create a sfpkg:\n",
       "\n",
       "```python\n",
       "Create SFPKG that needs to be uploaded to Azure Storage Blob Container\n",
       "microservices_sfpkg = zipfile.ZipFile(\n",
       "    self.microservices_app_package_name, 'w', zipfile.ZIP_DEFLATED)\n",
       "package_length = len(self.microservices_app_package_path)\n",
       "\n",
       "for root, dirs, files in os.walk(self.microservices_app_package_path):\n",
       "    root_folder = root[package_length:]\n",
       "    for file in files:\n",
       "        microservices_sfpkg.write(os.path.join(\n",
       "            root, file), os.path.join(root_folder, file))\n",
       "\n",
       "microservices_sfpkg.close()\n",
       "```\n",
       "\n",
       "Virtual machine OS automatic upgrade configuration\n",
       "\n",
       "Upgrading your virtual machines is a user initiated operation, and it is recommended that you enable virtual machine scale set automatic image upgrades for your Service Fabric cluster node patch management. Patch Orchestration Application (POA) is an alternative solution that is intended for non-Azure hosted clusters. Although POA can be used in Azure, hosting it requires more management than simply enabling scale set automatic OS image upgrades. The following are the virtual machine scale set Resource Manager template properties to enable automtic OS upgrades:\n",
       "\n",
       "```json\n",
       "\"upgradePolicy\": {\n",
       "   \"mode\": \"Automatic\",\n",
       "   \"automaticOSUpgradePolicy\": {\n",
       "        \"enableAutomaticOSUpgrade\": true,\n",
       "        \"disableAutomaticRollback\": false\n",
       "    }\n",
       "},\n",
       "```\n",
       "When using automatic OS upgrades with Service Fabric, the new OS image is rolled out one Update Domain at a time to maintain high availability of the services running in Service Fabric. To utilize Automatic OS Upgrades in Service Fabric, your cluster must be configured to use the Silver Durability Tier or higher.\n",
       "\n",
       "Ensure the following registry key is set to false to prevent your windows host machines from initiating uncoordinated updates: HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\WindowsUpdate\\AU.\n",
       "\n",
       "Set the following virtual machine scale set template properties to disable Windows Update:\n",
       "```json\n",
       "\"osProfile\": {\n",
       "        \"computerNamePrefix\": \"{vmss-name}\",\n",
       "        \"adminUsername\": \"{your-username}\",\n",
       "        \"secrets\": [],\n",
       "        \"windowsConfiguration\": {\n",
       "          \"provisionVMAgent\": true,\n",
       "          \"enableAutomaticUpdates\": false\n",
       "        }\n",
       "      },\n",
       "```\n",
       "\n",
       "Service Fabric cluster upgrade configuration\n",
       "\n",
       "The following is the Service Fabric cluster template property to enable automatic upgrade:\n",
       "\n",
       "```json\n",
       "\"upgradeMode\": \"Automatic\",\n",
       "```\n",
       "\n",
       "To manually upgrade your cluster, download the cab/deb distribution to a cluster virtual machine, and then invoke the following PowerShell:\n",
       "\n",
       "```powershell\n",
       "Copy-ServiceFabricClusterPackage -Code -CodePackagePath <\"local_VM_path_to_msi\"> -CodePackagePathInImageStore ServiceFabric.msi -ImageStoreConnectionString \"fabric:ImageStore\"\n",
       "Register-ServiceFabricClusterPackage -Code -CodePackagePath \"ServiceFabric.msi\"\n",
       "Start-ServiceFabricClusterUpgrade -Code -CodePackageVersion <\"msi_code_version\">\n",
       "```\n",
       "\n",
       "Next steps\n",
       "\n",
       "* Create a cluster on VMs or computers running Windows Server: Service Fabric cluster creation for Windows Server\n",
       "* Create a cluster on VMs or computers running Linux: Create a Linux cluster\n",
       "* Learn about Service Fabric support options</td><td>https://learn.microsoft.com/en-us/azure/service-fabric/service-fabric-best-practices-infrastructure-as-code.md</td><td>2026-01-08T05:31:03.035457Z</td></tr><tr><td>service-fabric/service-fabric-reliable-actors-access-save-remove-state.md</td><td>azure-compute-docs</td><td>service-fabric</td><td>service-fabric-reliable-actors-access-save-remove-state</td><td>---\n",
       "title: Manage Azure Service Fabric state \n",
       "description: Learn about accessing, saving, and removing state for an Azure Service Fabric Reliable Actor, and considerations when designing an application.\n",
       "ms.topic: how-to\n",
       "ms.author: tomcassidy\n",
       "author: tomvcassidy\n",
       "ms.service: azure-service-fabric\n",
       "services: service-fabric\n",
       "ms.date: 07/11/2022\n",
       "Customer intent: As a developer working with Reliable Actors, I want to manage actor state through access, saving, and removal methods, so that I can ensure data consistency and reliability in my cloud-based applications.\n",
       "---\n",
       "\n",
       "Access, save, and remove Reliable Actors state\n",
       "Reliable Actors are single-threaded objects that can encapsulate both logic and state and maintain state reliably. Every actor instance has its own state manager: a dictionary-like data structure that reliably stores key/value pairs. The state manager is a wrapper around a state provider. You can use it to store data regardless of which persistence setting is used.\n",
       "\n",
       "State manager keys must be strings. Values are generic and can be any type, including custom types. Values stored in the state manager must be data contract serializable because they might be transmitted over the network to other nodes during replication and might be written to disk, depending on an actor's state persistence setting.\n",
       "\n",
       "The state manager exposes common dictionary methods for managing state, similar to those found in Reliable Dictionary.\n",
       "\n",
       "For information, see best practices in managing actor state.\n",
       "\n",
       "Access state\n",
       "State is accessed through the state manager by key. State manager methods are all asynchronous because they might require disk I/O when actors have persisted state. Upon first access, state objects are cached in memory. Repeat access operations access objects directly from memory and return synchronously without incurring disk I/O or asynchronous context-switching overhead. A state object is removed from the cache in the following cases:\n",
       "\n",
       "* An actor method throws an unhandled exception after it retrieves an object from the state manager.\n",
       "* An actor is reactivated, either after being deactivated or after failure.\n",
       "* The state provider pages state to disk. This behavior depends on the state provider implementation. The default state provider for the `Persisted` setting has this behavior.\n",
       "\n",
       "You can retrieve state by using a standard *Get* operation that throws `KeyNotFoundException`(C#) or `NoSuchElementException`(Java) if an entry does not exist for the key:\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public Task<int> GetCountAsync()\n",
       "    {\n",
       "        return this.StateManager.GetStateAsync<int>(\"MyState\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture<Integer> getCountAsync()\n",
       "    {\n",
       "        return this.stateManager().getStateAsync(\"MyState\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "You can also retrieve state by using a *TryGet* method that does not throw if an entry does not exist for a key:\n",
       "\n",
       "```csharp\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public async Task<int> GetCountAsync()\n",
       "    {\n",
       "        ConditionalValue<int> result = await this.StateManager.TryGetStateAsync<int>(\"MyState\");\n",
       "        if (result.HasValue)\n",
       "        {\n",
       "            return result.Value;\n",
       "        }\n",
       "\n",
       "        return 0;\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture<Integer> getCountAsync()\n",
       "    {\n",
       "        return this.stateManager().<Integer>tryGetStateAsync(\"MyState\").thenApply(result -> {\n",
       "            if (result.hasValue()) {\n",
       "                return result.getValue();\n",
       "            } else {\n",
       "                return 0;\n",
       "            });\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "Save state\n",
       "The state manager retrieval methods return a reference to an object in local memory. Modifying this object in local memory alone does not cause it to be saved durably. When an object is retrieved from the state manager and modified, it must be reinserted into the state manager to be saved durably.\n",
       "\n",
       "You can insert state by using an unconditional *Set*, which is the equivalent of the `dictionary[\"key\"] = value` syntax:\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public Task SetCountAsync(int value)\n",
       "    {\n",
       "        return this.StateManager.SetStateAsync<int>(\"MyState\", value);\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture setCountAsync(int value)\n",
       "    {\n",
       "        return this.stateManager().setStateAsync(\"MyState\", value);\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "You can add state by using an *Add* method. This method throws `InvalidOperationException`(C#) or `IllegalStateException`(Java) when it tries to add a key that already exists.\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public Task AddCountAsync(int value)\n",
       "    {\n",
       "        return this.StateManager.AddStateAsync<int>(\"MyState\", value);\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture addCountAsync(int value)\n",
       "    {\n",
       "        return this.stateManager().addOrUpdateStateAsync(\"MyState\", value, (key, old_value) -> old_value + value);\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "You can also add state by using a *TryAdd* method. This method does not throw when it tries to add a key that already exists.\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public async Task AddCountAsync(int value)\n",
       "    {\n",
       "        bool result = await this.StateManager.TryAddStateAsync<int>(\"MyState\", value);\n",
       "\n",
       "        if (result)\n",
       "        {\n",
       "            // Added successfully!\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture addCountAsync(int value)\n",
       "    {\n",
       "        return this.stateManager().tryAddStateAsync(\"MyState\", value).thenApply((result)->{\n",
       "            if(result)\n",
       "            {\n",
       "                // Added successfully!\n",
       "            }\n",
       "        });\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "At the end of an actor method, the state manager automatically saves any values that have been added or modified by an insert or update operation. A \"save\" can include persisting to disk and replication, depending on the settings used. Values that have not been modified are not persisted or replicated. If no values have been modified, the save operation does nothing. If saving fails, the modified state is discarded and the original state is reloaded.\n",
       "\n",
       "You can also save state manually by calling the `SaveStateAsync` method on the actor base:\n",
       "\n",
       "```csharp\n",
       "async Task IMyActor.SetCountAsync(int count)\n",
       "{\n",
       "    await this.StateManager.AddOrUpdateStateAsync(\"count\", count, (key, value) => count > value ? count : value);\n",
       "\n",
       "    await this.SaveStateAsync();\n",
       "}\n",
       "```\n",
       "```Java\n",
       "interface MyActor {\n",
       "    CompletableFuture setCountAsync(int count)\n",
       "    {\n",
       "        this.stateManager().addOrUpdateStateAsync(\"count\", count, (key, value) -> count > value ? count : value).thenApply();\n",
       "\n",
       "        this.stateManager().saveStateAsync().thenApply();\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "Remove state\n",
       "You can remove state permanently from an actor's state manager by calling the *Remove* method. This method throws `KeyNotFoundException`(C#) or `NoSuchElementException`(Java) when it tries to remove a key that doesn't exist.\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public Task RemoveCountAsync()\n",
       "    {\n",
       "        return this.StateManager.RemoveStateAsync(\"MyState\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture removeCountAsync()\n",
       "    {\n",
       "        return this.stateManager().removeStateAsync(\"MyState\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "You can also remove state permanently by using the *TryRemove* method. This method does not throw when it tries to remove a key that does not exist.\n",
       "\n",
       "```csharp\n",
       "[StatePersistence(StatePersistence.Persisted)]\n",
       "class MyActor : Actor, IMyActor\n",
       "{\n",
       "    public MyActor(ActorService actorService, ActorId actorId)\n",
       "        : base(actorService, actorId)\n",
       "    {\n",
       "    }\n",
       "\n",
       "    public async Task RemoveCountAsync()\n",
       "    {\n",
       "        bool result = await this.StateManager.TryRemoveStateAsync(\"MyState\");\n",
       "\n",
       "        if (result)\n",
       "        {\n",
       "            // State removed!\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```\n",
       "```Java\n",
       "@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\n",
       "class MyActorImpl extends FabricActor implements  MyActor\n",
       "{\n",
       "    public MyActorImpl(ActorService actorService, ActorId actorId)\n",
       "    {\n",
       "        super(actorService, actorId);\n",
       "    }\n",
       "\n",
       "    public CompletableFuture removeCountAsync()\n",
       "    {\n",
       "        return this.stateManager().tryRemoveStateAsync(\"MyState\").thenApply((result)->{\n",
       "            if(result)\n",
       "            {\n",
       "                // State removed!\n",
       "            }\n",
       "        });\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "Next steps\n",
       "\n",
       "State that's stored in Reliable Actors must be serialized before it's written to disk and replicated for high availability. Learn more about Actor type serialization.\n",
       "\n",
       "Next, learn more about Actor diagnostics and performance monitoring.</td><td>https://learn.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-access-save-remove-state.md</td><td>2026-01-08T05:31:03.036412Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "service-fabric/service-fabric-best-practices-infrastructure-as-code.md",
         "azure-compute-docs",
         "service-fabric",
         "service-fabric-best-practices-infrastructure-as-code",
         "---\ntitle: Azure Service Fabric infrastructure as Code Best Practices\ndescription: Best practices and design considerations for managing Azure Service Fabric as infrastructure as code.\nms.topic: concept-article\nms.author: tomcassidy\nauthor: tomvcassidy\nms.service: azure-service-fabric\nservices: service-fabric\nms.date: 07/14/2022\nCustomer intent: \"As a cloud administrator, I want to utilize Infrastructure as Code to deploy and manage Azure Service Fabric clusters, so that I can ensure consistent and efficient resource configuration and maintenance.\"\n---\n\nInfrastructure as code\n\nIn a production scenario, create Azure Service Fabric clusters using Resource Manager templates. Resource Manager templates provide greater control of resource properties and ensure that you have a consistent resource model.\n\nSample Resource Manager templates are available for Windows and Linux in the Azure samples on GitHub. These templates can be used as a starting point for your cluster template. Download `azuredeploy.json` and `azuredeploy.parameters.json` and edit them to meet your custom requirements.\n\n!INCLUDE [updated-for-az]\n\nTo deploy the `azuredeploy.json` and `azuredeploy.parameters.json` templates you downloaded above, use the following Azure CLI commands:\n\n```azurecli\nResourceGroupName=\"sfclustergroup\"\nLocation=\"westus\"\n\naz group create --name $ResourceGroupName --location $Location \naz deployment group create --name $ResourceGroupName  --template-file azuredeploy.json --parameters @azuredeploy.parameters.json\n```\n\nCreating a resource using PowerShell\n\n```powershell\n$ResourceGroupName=\"sfclustergroup\"\n$Location=\"westus\"\n$Template=\"azuredeploy.json\"\n$Parameters=\"azuredeploy.parameters.json\"\n\nNew-AzResourceGroup -Name $ResourceGroupName -Location $Location\nNew-AzResourceGroupDeployment -Name $ResourceGroupName -TemplateFile $Template -TemplateParameterFile $Parameters\n```\n\nService Fabric resources\n\nYou can deploy applications and services onto your Service Fabric cluster via Azure Resource Manager. See Manage applications and services as Azure Resource Manager resources for details. The following are best practice Service Fabric application specific resources to include in your  Resource Manager template resources.\n\n```json\n{\n    \"apiVersion\": \"2019-03-01\",\n    \"type\": \"Microsoft.ServiceFabric/clusters/applicationTypes\",\n    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationTypeName'))]\",\n    \"location\": \"[variables('clusterLocation')]\",\n},\n{\n    \"apiVersion\": \"2019-03-01\",\n    \"type\": \"Microsoft.ServiceFabric/clusters/applicationTypes/versions\",\n    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationTypeName'), '/', parameters('applicationTypeVersion'))]\",\n    \"location\": \"[variables('clusterLocation')]\",\n},\n{\n    \"apiVersion\": \"2019-03-01\",\n    \"type\": \"Microsoft.ServiceFabric/clusters/applications\",\n    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationName'))]\",\n    \"location\": \"[variables('clusterLocation')]\",\n},\n{\n    \"apiVersion\": \"2019-03-01\",\n    \"type\": \"Microsoft.ServiceFabric/clusters/applications/services\",\n    \"name\": \"[concat(parameters('clusterName'), '/', parameters('applicationName'), '/', parameters('serviceName'))]\",\n    \"location\": \"[variables('clusterLocation')]\"\n}\n```\n\nTo deploy your application using Azure Resource Manager, you first must create a sfpkg Service Fabric Application package. The following Python script is an example of how to create a sfpkg:\n\n```python\nCreate SFPKG that needs to be uploaded to Azure Storage Blob Container\nmicroservices_sfpkg = zipfile.ZipFile(\n    self.microservices_app_package_name, 'w', zipfile.ZIP_DEFLATED)\npackage_length = len(self.microservices_app_package_path)\n\nfor root, dirs, files in os.walk(self.microservices_app_package_path):\n    root_folder = root[package_length:]\n    for file in files:\n        microservices_sfpkg.write(os.path.join(\n            root, file), os.path.join(root_folder, file))\n\nmicroservices_sfpkg.close()\n```\n\nVirtual machine OS automatic upgrade configuration\n\nUpgrading your virtual machines is a user initiated operation, and it is recommended that you enable virtual machine scale set automatic image upgrades for your Service Fabric cluster node patch management. Patch Orchestration Application (POA) is an alternative solution that is intended for non-Azure hosted clusters. Although POA can be used in Azure, hosting it requires more management than simply enabling scale set automatic OS image upgrades. The following are the virtual machine scale set Resource Manager template properties to enable automtic OS upgrades:\n\n```json\n\"upgradePolicy\": {\n   \"mode\": \"Automatic\",\n   \"automaticOSUpgradePolicy\": {\n        \"enableAutomaticOSUpgrade\": true,\n        \"disableAutomaticRollback\": false\n    }\n},\n```\nWhen using automatic OS upgrades with Service Fabric, the new OS image is rolled out one Update Domain at a time to maintain high availability of the services running in Service Fabric. To utilize Automatic OS Upgrades in Service Fabric, your cluster must be configured to use the Silver Durability Tier or higher.\n\nEnsure the following registry key is set to false to prevent your windows host machines from initiating uncoordinated updates: HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\WindowsUpdate\\AU.\n\nSet the following virtual machine scale set template properties to disable Windows Update:\n```json\n\"osProfile\": {\n        \"computerNamePrefix\": \"{vmss-name}\",\n        \"adminUsername\": \"{your-username}\",\n        \"secrets\": [],\n        \"windowsConfiguration\": {\n          \"provisionVMAgent\": true,\n          \"enableAutomaticUpdates\": false\n        }\n      },\n```\n\nService Fabric cluster upgrade configuration\n\nThe following is the Service Fabric cluster template property to enable automatic upgrade:\n\n```json\n\"upgradeMode\": \"Automatic\",\n```\n\nTo manually upgrade your cluster, download the cab/deb distribution to a cluster virtual machine, and then invoke the following PowerShell:\n\n```powershell\nCopy-ServiceFabricClusterPackage -Code -CodePackagePath <\"local_VM_path_to_msi\"> -CodePackagePathInImageStore ServiceFabric.msi -ImageStoreConnectionString \"fabric:ImageStore\"\nRegister-ServiceFabricClusterPackage -Code -CodePackagePath \"ServiceFabric.msi\"\nStart-ServiceFabricClusterUpgrade -Code -CodePackageVersion <\"msi_code_version\">\n```\n\nNext steps\n\n* Create a cluster on VMs or computers running Windows Server: Service Fabric cluster creation for Windows Server\n* Create a cluster on VMs or computers running Linux: Create a Linux cluster\n* Learn about Service Fabric support options",
         "https://learn.microsoft.com/en-us/azure/service-fabric/service-fabric-best-practices-infrastructure-as-code.md",
         "2026-01-08T05:31:03.035457Z"
        ],
        [
         "service-fabric/service-fabric-reliable-actors-access-save-remove-state.md",
         "azure-compute-docs",
         "service-fabric",
         "service-fabric-reliable-actors-access-save-remove-state",
         "---\ntitle: Manage Azure Service Fabric state \ndescription: Learn about accessing, saving, and removing state for an Azure Service Fabric Reliable Actor, and considerations when designing an application.\nms.topic: how-to\nms.author: tomcassidy\nauthor: tomvcassidy\nms.service: azure-service-fabric\nservices: service-fabric\nms.date: 07/11/2022\nCustomer intent: As a developer working with Reliable Actors, I want to manage actor state through access, saving, and removal methods, so that I can ensure data consistency and reliability in my cloud-based applications.\n---\n\nAccess, save, and remove Reliable Actors state\nReliable Actors are single-threaded objects that can encapsulate both logic and state and maintain state reliably. Every actor instance has its own state manager: a dictionary-like data structure that reliably stores key/value pairs. The state manager is a wrapper around a state provider. You can use it to store data regardless of which persistence setting is used.\n\nState manager keys must be strings. Values are generic and can be any type, including custom types. Values stored in the state manager must be data contract serializable because they might be transmitted over the network to other nodes during replication and might be written to disk, depending on an actor's state persistence setting.\n\nThe state manager exposes common dictionary methods for managing state, similar to those found in Reliable Dictionary.\n\nFor information, see best practices in managing actor state.\n\nAccess state\nState is accessed through the state manager by key. State manager methods are all asynchronous because they might require disk I/O when actors have persisted state. Upon first access, state objects are cached in memory. Repeat access operations access objects directly from memory and return synchronously without incurring disk I/O or asynchronous context-switching overhead. A state object is removed from the cache in the following cases:\n\n* An actor method throws an unhandled exception after it retrieves an object from the state manager.\n* An actor is reactivated, either after being deactivated or after failure.\n* The state provider pages state to disk. This behavior depends on the state provider implementation. The default state provider for the `Persisted` setting has this behavior.\n\nYou can retrieve state by using a standard *Get* operation that throws `KeyNotFoundException`(C#) or `NoSuchElementException`(Java) if an entry does not exist for the key:\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public Task<int> GetCountAsync()\n    {\n        return this.StateManager.GetStateAsync<int>(\"MyState\");\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture<Integer> getCountAsync()\n    {\n        return this.stateManager().getStateAsync(\"MyState\");\n    }\n}\n```\n\nYou can also retrieve state by using a *TryGet* method that does not throw if an entry does not exist for a key:\n\n```csharp\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public async Task<int> GetCountAsync()\n    {\n        ConditionalValue<int> result = await this.StateManager.TryGetStateAsync<int>(\"MyState\");\n        if (result.HasValue)\n        {\n            return result.Value;\n        }\n\n        return 0;\n    }\n}\n```\n```Java\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture<Integer> getCountAsync()\n    {\n        return this.stateManager().<Integer>tryGetStateAsync(\"MyState\").thenApply(result -> {\n            if (result.hasValue()) {\n                return result.getValue();\n            } else {\n                return 0;\n            });\n    }\n}\n```\n\nSave state\nThe state manager retrieval methods return a reference to an object in local memory. Modifying this object in local memory alone does not cause it to be saved durably. When an object is retrieved from the state manager and modified, it must be reinserted into the state manager to be saved durably.\n\nYou can insert state by using an unconditional *Set*, which is the equivalent of the `dictionary[\"key\"] = value` syntax:\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public Task SetCountAsync(int value)\n    {\n        return this.StateManager.SetStateAsync<int>(\"MyState\", value);\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture setCountAsync(int value)\n    {\n        return this.stateManager().setStateAsync(\"MyState\", value);\n    }\n}\n```\n\nYou can add state by using an *Add* method. This method throws `InvalidOperationException`(C#) or `IllegalStateException`(Java) when it tries to add a key that already exists.\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public Task AddCountAsync(int value)\n    {\n        return this.StateManager.AddStateAsync<int>(\"MyState\", value);\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture addCountAsync(int value)\n    {\n        return this.stateManager().addOrUpdateStateAsync(\"MyState\", value, (key, old_value) -> old_value + value);\n    }\n}\n```\n\nYou can also add state by using a *TryAdd* method. This method does not throw when it tries to add a key that already exists.\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public async Task AddCountAsync(int value)\n    {\n        bool result = await this.StateManager.TryAddStateAsync<int>(\"MyState\", value);\n\n        if (result)\n        {\n            // Added successfully!\n        }\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture addCountAsync(int value)\n    {\n        return this.stateManager().tryAddStateAsync(\"MyState\", value).thenApply((result)->{\n            if(result)\n            {\n                // Added successfully!\n            }\n        });\n    }\n}\n```\n\nAt the end of an actor method, the state manager automatically saves any values that have been added or modified by an insert or update operation. A \"save\" can include persisting to disk and replication, depending on the settings used. Values that have not been modified are not persisted or replicated. If no values have been modified, the save operation does nothing. If saving fails, the modified state is discarded and the original state is reloaded.\n\nYou can also save state manually by calling the `SaveStateAsync` method on the actor base:\n\n```csharp\nasync Task IMyActor.SetCountAsync(int count)\n{\n    await this.StateManager.AddOrUpdateStateAsync(\"count\", count, (key, value) => count > value ? count : value);\n\n    await this.SaveStateAsync();\n}\n```\n```Java\ninterface MyActor {\n    CompletableFuture setCountAsync(int count)\n    {\n        this.stateManager().addOrUpdateStateAsync(\"count\", count, (key, value) -> count > value ? count : value).thenApply();\n\n        this.stateManager().saveStateAsync().thenApply();\n    }\n}\n```\n\nRemove state\nYou can remove state permanently from an actor's state manager by calling the *Remove* method. This method throws `KeyNotFoundException`(C#) or `NoSuchElementException`(Java) when it tries to remove a key that doesn't exist.\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public Task RemoveCountAsync()\n    {\n        return this.StateManager.RemoveStateAsync(\"MyState\");\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture removeCountAsync()\n    {\n        return this.stateManager().removeStateAsync(\"MyState\");\n    }\n}\n```\n\nYou can also remove state permanently by using the *TryRemove* method. This method does not throw when it tries to remove a key that does not exist.\n\n```csharp\n[StatePersistence(StatePersistence.Persisted)]\nclass MyActor : Actor, IMyActor\n{\n    public MyActor(ActorService actorService, ActorId actorId)\n        : base(actorService, actorId)\n    {\n    }\n\n    public async Task RemoveCountAsync()\n    {\n        bool result = await this.StateManager.TryRemoveStateAsync(\"MyState\");\n\n        if (result)\n        {\n            // State removed!\n        }\n    }\n}\n```\n```Java\n@StatePersistenceAttribute(statePersistence = StatePersistence.Persisted)\nclass MyActorImpl extends FabricActor implements  MyActor\n{\n    public MyActorImpl(ActorService actorService, ActorId actorId)\n    {\n        super(actorService, actorId);\n    }\n\n    public CompletableFuture removeCountAsync()\n    {\n        return this.stateManager().tryRemoveStateAsync(\"MyState\").thenApply((result)->{\n            if(result)\n            {\n                // State removed!\n            }\n        });\n    }\n}\n```\n\nNext steps\n\nState that's stored in Reliable Actors must be serialized before it's written to disk and replicated for high availability. Learn more about Actor type serialization.\n\nNext, learn more about Actor diagnostics and performance monitoring.",
         "https://learn.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-access-save-remove-state.md",
         "2026-01-08T05:31:03.036412Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "doc_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "raw_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "url",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingest_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_spark_row_data():\n",
    "    article_path = Path(TARGET_ARTICLE_PATH)\n",
    "    rows = []\n",
    "    min_length = 500  # skip stubs / TOCs\n",
    "\n",
    "    for md_file in article_path.rglob(\"*.md\"):\n",
    "        try:\n",
    "            with open(md_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                raw_md = f.read()\n",
    "\n",
    "            cleaned = clean_markdown(raw_md)\n",
    "\n",
    "            if len(cleaned) < min_length:\n",
    "                continue\n",
    "\n",
    "            rel_path = str(md_file.relative_to(article_path))\n",
    "            category = rel_path.split(\"/\", 1)[0] if \"/\" in rel_path else rel_path\n",
    "\n",
    "            rows.append(Row(\n",
    "                doc_id=rel_path,\n",
    "                source=\"azure-compute-docs\",\n",
    "                category=category,  # e.g. virtual-machines\n",
    "                title=md_file.stem,\n",
    "                raw_text=cleaned,\n",
    "                url=f\"https://learn.microsoft.com/en-us/azure/{md_file.relative_to(TARGET_ARTICLE_PATH)}\",\n",
    "                ingest_time=datetime.utcnow()\n",
    "            ))\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fail-safe: skip bad files\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "rows = prepare_spark_row_data()\n",
    "docs_df = spark.createDataFrame(rows)\n",
    "\n",
    "display(docs_df.limit(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12bc2e2-ccc2-4b53-baf6-b3eb0444be45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.connect.session.SparkSession"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Databricks notebooks, spark is: a pre-initialized SparkSession object that Databricks creates for you.\n",
    "\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d093b6-b775-45a9-89be-365d5ae08572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_delta_table(docs_df):\n",
    "\n",
    "    # write to warehouse\n",
    "    (\n",
    "        docs_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"{DEFAULT_CATELOG_NAME}.default.{TABLE_NAME}\")\n",
    "    )\n",
    "\n",
    "save_to_delta_table(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bef1e7-db22-4656-a538-4577c7536870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>1751</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1751
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "__autoGeneratedAlias": "true"
            },
            "name": "count(1)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "--- Validate data is stored into the table successfully\n",
    "SELECT COUNT(*) FROM databricks_rag_demo.default.raw_azure_compute_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7275fbcf-3707-4881-b407-bb1532dcf50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- delete this table if needed, this will clean up the environment\n",
    "DROP TABLE databricks_rag_demo.default.raw_azure_compute_docs;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8004489376597640,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_azure_compute_docs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}