{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a1b4e-da0a-49e9-9197-a88fabe69628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have one row per chunk now. In this notebook we will convert each chunk's text into a numeric vector (embedding) so later can be used for: We have one row per chunk now. In this notebook we will convert each chunk's text into a numeric vector (embedding) so later can be used for: search by semantic similarity, get the retrieve relevant chunks and feed them to an LLM for RAG.\n",
    " \n",
    "It reads chunk-level documents from Unity Catalog, generates text embeddings using an external embedding model, and stores the resulting vectors as a Delta table for semantic search and retrieval-augmented generation (RAG).\n",
    "\n",
    "Input and Output:\n",
    "- Input table: databricks_rag_demo.default.azure_compute_doc_chunks\n",
    "- Output table: databricks_rag_demo.default.azure_compute_doc_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deafaf35-daba-4967-a20b-1f960bc77db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Embedding strategy (important decisions)\n",
    "\n",
    "For this project we will:\n",
    "-   Use OpenAI-style embeddings (works with OpenAI or Azure OpenAI)\n",
    "-   Generate embeddings in batches (not per row)\n",
    "- Store embeddings as: ARRAY<FLOAT> (simple, portable)\n",
    "- Keep metadata alongside vectors\n",
    "\n",
    "This is the most common production pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07bf7828-9af5-4d6f-bb94-731b5378ed10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_install_deps_and_restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df36d41c-2841-4ece-b7e4-3e02b3ce4da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run ./00_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845dee57-cede-4b09-810f-af560fd0f044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94acb27c-4c79-4c2f-b481-94a17c2cead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_init_openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f62e409-33ff-4a0b-b46f-077a7f2e0a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b195736-c588-4176-b3a2-82ce9cdeeaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks_df = spark.table(CHUNKS_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b744fae-fbb4-4bbc-b859-e3fad5f321a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>doc_id</th><th>category</th><th>chunk_id</th><th>chunk_text</th></tr></thead><tbody><tr><td>virtual-machines/hc-series-performance.md</td><td>virtual-machines</td><td>e0a4a830db8bf7306f91c8934db2e3eb86a108e1367dcd3ee553adb9082f5212</td><td>title hc series vm size performance description learn about performance testing results for hc series vm sizes in azure ms service azure virtual machines ms subservice hpc ms topic concept article ms date 07 25 2024 ms reviewer cynthn ms author padmalathas author cynthn customer intent as a cloud architect i want to analyze the performance results of hc series vm sizes so that i can select the optimal configuration for my high performance computing workloads hc series virtual machine sizes applies to heavy_check_mark linux vms heavy_check_mark windows vms heavy_check_mark flexible scale sets heavy_check_mark uniform scale sets several performance tests have been run on hc series sizes the following are some of the results of this performance testing workload hb stream triad 190 gb s intel mlc avx 512 high performance linpack hpl 3520 gigaflops rpeak 2970 gigaflops rmax rdma latency bandwidth 1 05 microseconds 96 8 gb s fio on local nvme ssd 1 3 gb s reads 900 mb s writes ior on 4 azure premium ssd p30 managed disks raid0 780 mb s reads 780 mb writes mpi latency mpi latency test from the osu microbenchmark suite is run sample scripts are on github bash bin mpirun_rsh np 2 hostfile hostfile mv2_cpu_mapping insert core osu_latency mpi bandwidth mpi bandwidth test from the osu microbenchmark suite is run sample scripts are on github bash mvapich2 2 3 install bin mpirun_rsh np 2 hostfile hostfile mv2_cpu_mapping insert core mvapich2 2 3 osu_benchmarks mpi pt2pt osu_bw mellanox perftest the mellanox perftest package has many infiniband tests such as latency ib_send_lat and bandwidth ib_send_bw an example command is below console numactl physcpubind insert core ib_send_lat a next steps read about the latest announcements hpc workload examples and performance results at the azure compute tech community blogs for a higher level architectural view of running hpc workloads see high performance computing hpc on azure</td></tr><tr><td>virtual-machines/premium-storage-performance.md</td><td>virtual-machines</td><td>d289e62cefb20a7fac3323cc6fccb65d54c83a66cddd95e1b68b00c3e473ed43</td><td>title azure premium storage design for high performance description design high performance apps by using azure premium ssd managed disks azure premium storage offers high performance low latency disk support for i o intensive workloads running on azure vms author roygara ms service azure disk storage ms custom linux related content ms topic concept article ms date 06 29 2021 ms author rogarana customer intent as a developer i want to optimize application performance on premium storage so that i can ensure my high performance apps meet the demands of i o intensive workloads efficiently azure premium storage design for high performance applies to heavy_check_mark linux vms heavy_check_mark windows vms heavy_check_mark flexible scale sets heavy_check_mark uniform scale sets this article provides guidelines for building high performance applications by using azure premium storage you can use the instructions provided in this document combined with performance best practices applicable to technologies used by your application to illustrate the guidelines we use sql server running on premium storage as an example throughout this document while we address performance scenarios for the storage layer in this article you need to optimize the application layer for example if you re hosting a sharepoint farm on premium storage you can use the sql server examples from this article to optimize the database server you can also optimize the sharepoint farm s web server and application server to get the most performance this article helps to answer the following common questions about optimizing application performance on premium storage how can you measure your application performance why aren t you seeing expected high performance which factors influence your application performance on premium storage how do these factors influence performance of your application on premium storage how can you optimize for input output operations per second iops bandwidth and latency we provide these guidelines specifically for premium storage because workloads running on premium storage are highly performance sensitive we provide examples where appropriate you can also apply some of these guidelines to applications running on infrastructure as a service iaas vms with standard storage disks note sometimes what appears to be a disk performance issue is actually a network bottleneck in these situations you should optimize your network performance if you re looking to benchmark your disk see the following articles for linux benchmark your application on azure disk storage for windows benchmark a disk if your vm supports</td></tr><tr><td>virtual-machines/premium-storage-performance.md</td><td>virtual-machines</td><td>7eb206e0baf9dccd72fd1c0d1c1db060d37cdd64f5b5f190080eb48111b91d8e</td><td>to be a disk performance issue is actually a network bottleneck in these situations you should optimize your network performance if you re looking to benchmark your disk see the following articles for linux benchmark your application on azure disk storage for windows benchmark a disk if your vm supports accelerated networking make sure it s enabled if it s not enabled you can enable it on already deployed vms on both windows and linux before you begin if you re new to premium storage first read select an azure disk type for iaas vms and scalability targets for premium page blob storage accounts application performance indicators we assess whether an application is performing well or not by using performance indicators like how fast an application is processing a user request how much data an application is processing per request how many requests an application is processing in a specific period of time how long a user has to wait to get a response after submitting their request the technical terms for these performance indicators are iops throughput or bandwidth and latency in this section we discuss the common performance indicators in the context of premium storage in the section performance application checklist for disks you learn how to measure these performance indicators for your application later in optimize application performance you learn about the factors that affect these performance indicators and recommendations to optimize them iops iops is the number of requests that your application is sending to storage disks in one second an input output operation could be read or write sequential or random online transaction processing oltp applications like an online retail website need to process many concurrent user requests immediately the user requests are insert and update intensive database transactions which the application must process quickly for this reason oltp applications require very high iops oltp applications handle millions of small and random i o requests if you have such an application you must design the application infrastructure to optimize for iops for more information on all the factors to consider to get high iops see optimize application performance when you attach a premium storage disk to your high scale vm azure provisions for you a guaranteed number of iops according to the disk specification for example a p50 disk provisions 7 500 iops each high scale vm size also has a specific iops limit that</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "virtual-machines/hc-series-performance.md",
         "virtual-machines",
         "e0a4a830db8bf7306f91c8934db2e3eb86a108e1367dcd3ee553adb9082f5212",
         "title hc series vm size performance description learn about performance testing results for hc series vm sizes in azure ms service azure virtual machines ms subservice hpc ms topic concept article ms date 07 25 2024 ms reviewer cynthn ms author padmalathas author cynthn customer intent as a cloud architect i want to analyze the performance results of hc series vm sizes so that i can select the optimal configuration for my high performance computing workloads hc series virtual machine sizes applies to heavy_check_mark linux vms heavy_check_mark windows vms heavy_check_mark flexible scale sets heavy_check_mark uniform scale sets several performance tests have been run on hc series sizes the following are some of the results of this performance testing workload hb stream triad 190 gb s intel mlc avx 512 high performance linpack hpl 3520 gigaflops rpeak 2970 gigaflops rmax rdma latency bandwidth 1 05 microseconds 96 8 gb s fio on local nvme ssd 1 3 gb s reads 900 mb s writes ior on 4 azure premium ssd p30 managed disks raid0 780 mb s reads 780 mb writes mpi latency mpi latency test from the osu microbenchmark suite is run sample scripts are on github bash bin mpirun_rsh np 2 hostfile hostfile mv2_cpu_mapping insert core osu_latency mpi bandwidth mpi bandwidth test from the osu microbenchmark suite is run sample scripts are on github bash mvapich2 2 3 install bin mpirun_rsh np 2 hostfile hostfile mv2_cpu_mapping insert core mvapich2 2 3 osu_benchmarks mpi pt2pt osu_bw mellanox perftest the mellanox perftest package has many infiniband tests such as latency ib_send_lat and bandwidth ib_send_bw an example command is below console numactl physcpubind insert core ib_send_lat a next steps read about the latest announcements hpc workload examples and performance results at the azure compute tech community blogs for a higher level architectural view of running hpc workloads see high performance computing hpc on azure"
        ],
        [
         "virtual-machines/premium-storage-performance.md",
         "virtual-machines",
         "d289e62cefb20a7fac3323cc6fccb65d54c83a66cddd95e1b68b00c3e473ed43",
         "title azure premium storage design for high performance description design high performance apps by using azure premium ssd managed disks azure premium storage offers high performance low latency disk support for i o intensive workloads running on azure vms author roygara ms service azure disk storage ms custom linux related content ms topic concept article ms date 06 29 2021 ms author rogarana customer intent as a developer i want to optimize application performance on premium storage so that i can ensure my high performance apps meet the demands of i o intensive workloads efficiently azure premium storage design for high performance applies to heavy_check_mark linux vms heavy_check_mark windows vms heavy_check_mark flexible scale sets heavy_check_mark uniform scale sets this article provides guidelines for building high performance applications by using azure premium storage you can use the instructions provided in this document combined with performance best practices applicable to technologies used by your application to illustrate the guidelines we use sql server running on premium storage as an example throughout this document while we address performance scenarios for the storage layer in this article you need to optimize the application layer for example if you re hosting a sharepoint farm on premium storage you can use the sql server examples from this article to optimize the database server you can also optimize the sharepoint farm s web server and application server to get the most performance this article helps to answer the following common questions about optimizing application performance on premium storage how can you measure your application performance why aren t you seeing expected high performance which factors influence your application performance on premium storage how do these factors influence performance of your application on premium storage how can you optimize for input output operations per second iops bandwidth and latency we provide these guidelines specifically for premium storage because workloads running on premium storage are highly performance sensitive we provide examples where appropriate you can also apply some of these guidelines to applications running on infrastructure as a service iaas vms with standard storage disks note sometimes what appears to be a disk performance issue is actually a network bottleneck in these situations you should optimize your network performance if you re looking to benchmark your disk see the following articles for linux benchmark your application on azure disk storage for windows benchmark a disk if your vm supports"
        ],
        [
         "virtual-machines/premium-storage-performance.md",
         "virtual-machines",
         "7eb206e0baf9dccd72fd1c0d1c1db060d37cdd64f5b5f190080eb48111b91d8e",
         "to be a disk performance issue is actually a network bottleneck in these situations you should optimize your network performance if you re looking to benchmark your disk see the following articles for linux benchmark your application on azure disk storage for windows benchmark a disk if your vm supports accelerated networking make sure it s enabled if it s not enabled you can enable it on already deployed vms on both windows and linux before you begin if you re new to premium storage first read select an azure disk type for iaas vms and scalability targets for premium page blob storage accounts application performance indicators we assess whether an application is performing well or not by using performance indicators like how fast an application is processing a user request how much data an application is processing per request how many requests an application is processing in a specific period of time how long a user has to wait to get a response after submitting their request the technical terms for these performance indicators are iops throughput or bandwidth and latency in this section we discuss the common performance indicators in the context of premium storage in the section performance application checklist for disks you learn how to measure these performance indicators for your application later in optimize application performance you learn about the factors that affect these performance indicators and recommendations to optimize them iops iops is the number of requests that your application is sending to storage disks in one second an input output operation could be read or write sequential or random online transaction processing oltp applications like an online retail website need to process many concurrent user requests immediately the user requests are insert and update intensive database transactions which the application must process quickly for this reason oltp applications require very high iops oltp applications handle millions of small and random i o requests if you have such an application you must design the application infrastructure to optimize for iops for more information on all the factors to consider to get high iops see optimize application performance when you attach a premium storage disk to your high scale vm azure provisions for you a guaranteed number of iops according to the disk specification for example a p50 disk provisions 7 500 iops each high scale vm size also has a specific iops limit that"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "doc_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT doc_id, category, chunk_id, chunk_text FROM {CHUNKS_TABLE} LIMIT 3\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938eb47e-40fc-47a4-82ff-8dcb6c7efba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|doc_id                                         |category        |chunk_index|chunk_preview                                                                                                                                                                                           |\n+-----------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|virtual-machines/hc-series-performance.md      |virtual-machines|0          |title hc series vm size performance description learn about performance testing results for hc series vm sizes in azure ms service azure virtual machines ms subservice hpc ms topic concept article ms |\n|virtual-machines/premium-storage-performance.md|virtual-machines|0          |title azure premium storage design for high performance description design high performance apps by using azure premium ssd managed disks azure premium storage offers high performance low latency disk|\n|virtual-machines/premium-storage-performance.md|virtual-machines|1          |to be a disk performance issue is actually a network bottleneck in these situations you should optimize your network performance if you re looking to benchmark your disk see the following articles for|\n+-----------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# chunk_text is too long to show\n",
    "chunks_df.select(\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"chunk_index\",\n",
    "    F.substring(\"chunk_text\", 1, 200).alias(\"chunk_preview\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4031b183-7bc1-4e8c-9e5d-958203429643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How to create Azure OpenAI service and get a api key:\n",
    "- In Azure Portal: Create Azure OpenAI resource\n",
    "- Navigate to Foundry portal: Deploy a model of text-embedding-3-small or text-embedding-ada-002\n",
    "- Get:\n",
    "\t- Endpoint\n",
    "\t- API key\n",
    "\n",
    "\n",
    "How to store API key in notebook (one-time)\n",
    "- Workspace â†’ Secrets\n",
    "- Create scope: openai\n",
    "  - Key: OPENAI_API_KEY\n",
    "  - Value: your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0a4ebb-7ae3-457b-a30d-c68980f41904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 947 rows\n"
     ]
    }
   ],
   "source": [
    "# Collect chunks in manageable batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "rows = chunks_df.select(\n",
    "    \"chunk_id\",\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"chunk_index\",\n",
    "    \"chunk_text\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Found {len(rows)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fda7c37-f800-44aa-ad49-728fc87f0b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk:  0\nChunk:  64\nChunk:  128\nChunk:  192\nChunk:  256\nChunk:  320\nChunk:  384\nChunk:  448\nChunk:  512\nChunk:  576\nChunk:  640\nChunk:  704\nChunk:  768\nChunk:  832\nChunk:  896\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-3630405f303c421ba2ae3e6d17f8db46\", \"tr-f92470dbf8fb441ba29caffe543b2648\", \"tr-99fce6a425f141e280f96be4a39e37fc\", \"tr-2a766d608f1b471d8e993839896c64eb\", \"tr-c1b92d831b4b4ac59f699549214b3fe4\", \"tr-672700aa8c3548d8bf0d59fa25057def\", \"tr-3cb626b9217c4721a55ffbefe0590b67\", \"tr-390f28b14cc148ffa541e3d864313910\", \"tr-eac8d7a8ce8542cc979808363deaf7a8\", \"tr-dcba26e2afaf4972a65f5b483544fdc8\"]",
      "text/plain": [
       "[Trace(request_id=tr-3630405f303c421ba2ae3e6d17f8db46), Trace(request_id=tr-f92470dbf8fb441ba29caffe543b2648), Trace(request_id=tr-99fce6a425f141e280f96be4a39e37fc), Trace(request_id=tr-2a766d608f1b471d8e993839896c64eb), Trace(request_id=tr-c1b92d831b4b4ac59f699549214b3fe4), Trace(request_id=tr-672700aa8c3548d8bf0d59fa25057def), Trace(request_id=tr-3cb626b9217c4721a55ffbefe0590b67), Trace(request_id=tr-390f28b14cc148ffa541e3d864313910), Trace(request_id=tr-eac8d7a8ce8542cc979808363deaf7a8), Trace(request_id=tr-dcba26e2afaf4972a65f5b483544fdc8)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Below can run for a while depends on how many chunks\n",
    "#  Generate embeddings\n",
    "\n",
    "embedded_rows = []\n",
    "\n",
    "for i in range(0, len(rows), BATCH_SIZE):\n",
    "    print(\"Chunk: \", i)\n",
    "    batch = rows[i:i + BATCH_SIZE]\n",
    "    texts = [r.chunk_text for r in batch]\n",
    "\n",
    "    embeddings = embed_texts(texts)\n",
    "\n",
    "    for r, emb in zip(batch, embeddings):\n",
    "        embedded_rows.append((\n",
    "            r.chunk_id,\n",
    "            r.doc_id,\n",
    "            r.category,\n",
    "            r.title,\n",
    "            r.url,\n",
    "            r.chunk_index,\n",
    "            r.chunk_text,\n",
    "            emb\n",
    "        ))\n",
    "\n",
    "    # Below line is used during initial test to limit data: only include first 10 batches\n",
    "    # if i > 10*BATCH_SIZE: break\n",
    "\n",
    "    time.sleep(0.5)  # be polite to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70ff03e-9c18-46e0-93c5-aef7bec7842b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n| dim|\n+----+\n|1536|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings DataFrame\n",
    "\n",
    "embeddings_df = spark.createDataFrame(\n",
    "    embedded_rows,\n",
    "    schema=[\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"category\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"chunk_text\",\n",
    "        \"embedding\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check vector length:\n",
    "embeddings_df.select(F.size(\"embedding\").alias(\"dim\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb060d6-3188-473d-ac3a-47856914f57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    embeddings_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(EMB_TABLE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6164392d-5ba5-41dc-946c-7cb5de5a3015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>947</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         947
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) FROM {EMB_TABLE}\n",
    "\"\"\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db8f318-01fb-4533-9ace-921ec394e50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>embedding_dim</th></tr></thead><tbody><tr><td>virtual-machines</td><td>1536</td></tr><tr><td>virtual-machines</td><td>1536</td></tr><tr><td>virtual-machines</td><td>1536</td></tr><tr><td>virtual-machines</td><td>1536</td></tr><tr><td>virtual-machines</td><td>1536</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "virtual-machines",
         1536
        ],
        [
         "virtual-machines",
         1536
        ],
        [
         "virtual-machines",
         1536
        ],
        [
         "virtual-machines",
         1536
        ],
        [
         "virtual-machines",
         1536
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "embedding_dim",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT category, size(embedding) AS embedding_dim FROM {EMB_TABLE} LIMIT 5\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_generate_embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}