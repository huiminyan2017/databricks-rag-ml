{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a1b4e-da0a-49e9-9197-a88fabe69628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have one row per chunk now. In this notebook we will convert each chunk's text into a numeric vector (embedding) so later can be used for: We have one row per chunk now. In this notebook we will convert each chunk's text into a numeric vector (embedding) so later can be used for: \n",
    "- search by semantic similarity\n",
    "- get the retrieve relevant chunks\n",
    "- feed them to an LLM for RAG.\n",
    " \n",
    "It reads chunk-level documents from Unity Catalog, generates text embeddings using an external embedding model, and stores the resulting vectors as a Delta table for semantic search and retrieval-augmented generation (RAG).\n",
    "\n",
    "Input and Output:\n",
    "- Input table: databricks_rag_demo.default.azure_compute_doc_chunks\n",
    "- Output table: databricks_rag_demo.default.azure_compute_doc_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deafaf35-daba-4967-a20b-1f960bc77db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Embedding strategy (important decisions)\n",
    "\n",
    "For this project we will:\n",
    "-   Use OpenAI-style embeddings (works with OpenAI or Azure OpenAI)\n",
    "-   Generate embeddings in batches (not per row)\n",
    "- Store embeddings as: ARRAY<FLOAT> (simple, portable)\n",
    "- Keep metadata alongside vectors\n",
    "\n",
    "This is the most common production pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5210cfbd-005a-4b27-a6b9-ef3bbc69e8fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Disable mlflow autologging\n",
    "mlflow.autolog(disable=True)\n",
    "mlflow.openai.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bf7828-9af5-4d6f-bb94-731b5378ed10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_install_deps_and_restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df36d41c-2841-4ece-b7e4-3e02b3ce4da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run ./00_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845dee57-cede-4b09-810f-af560fd0f044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai<2.0.0,>=1.0.0\n  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\nCollecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.0.0)\n  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.0.0) (1.9.0)\nCollecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.0.0)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.0.0)\n  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from openai<2.0.0,>=1.0.0) (2.8.2)\nCollecting sniffio (from openai<2.0.0,>=1.0.0)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nCollecting tqdm>4 (from openai<2.0.0,>=1.0.0)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/57.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in /databricks/python3/lib/python3.12/site-packages (from openai<2.0.0,>=1.0.0) (4.11.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.0.0) (3.7)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0) (2024.6.2)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0) (2.20.1)\nDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/948.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m942.1/948.6 kB\u001B[0m \u001B[31m54.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m948.6/948.6 kB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/113.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m113.6/113.6 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/73.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.5/73.5 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.8/78.8 kB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/361.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m361.3/361.3 kB\u001B[0m \u001B[31m16.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nInstalling collected packages: tqdm, sniffio, jiter, h11, anyio, httpcore, httpx, openai\nSuccessfully installed anyio-4.12.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 openai-1.109.1 sniffio-1.3.1 tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-vectorsearch\n  Downloading databricks_vectorsearch-0.63-py3-none-any.whl.metadata (2.8 kB)\nCollecting deprecation>=2 (from databricks-vectorsearch)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: mlflow-skinny<4,>=2.11.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (2.19.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (4.24.1)\nRequirement already satisfied: requests>=2 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (2.32.2)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch) (24.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (2.2.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.30.0)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.1.37)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.27.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (6.0.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.5.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2024.6.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (2.35.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (3.17.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.48b0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.11.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (4.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<4,>=2.11.3->databricks-vectorsearch) (0.4.8)\nDownloading databricks_vectorsearch-0.63-py3-none-any.whl (19 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: deprecation, databricks-vectorsearch\nSuccessfully installed databricks-vectorsearch-0.63 deprecation-2.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nNew packages installed — restarting Python kernel...\n\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%run ./00_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94acb27c-4c79-4c2f-b481-94a17c2cead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_init_openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f62e409-33ff-4a0b-b46f-077a7f2e0a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b195736-c588-4176-b3a2-82ce9cdeeaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks_df = spark.table(CHUNKS_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b744fae-fbb4-4bbc-b859-e3fad5f321a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>doc_id</th><th>category</th><th>chunk_id</th><th>chunk_text</th></tr></thead><tbody><tr><td>container-instances/container-instances-quickstart.md</td><td>container-instances</td><td>270cb38d2abc6af424f9c11d46123bbfb03530b32545c638a1b734f1e9767434</td><td>title quickstart deploy docker container to container instance azure cli description in this quickstart you use the azure cli to quickly deploy a containerized web app that runs in an isolated azure container instance ms topic quickstart ms author tomcassidy author tomvcassidy ms service azure container instances services container instances ms date 11 17 2025 ms update cycle 180 days ms custom mvc devx track azurecli mode api customer intent as a developer i want to quickly deploy a docker container using the command line so that i can run my web application without managing complex orchestration platforms quickstart deploy a container instance in azure using the azure cli use azure container instances to run serverless docker containers in azure with simplicity and speed deploy an application to a container instance on demand when you don t need a full container orchestration platform like azure kubernetes service in this quickstart you use the azure cli to deploy an isolated docker container and make its application available with a fully qualified domain name fqdn a few seconds after you execute a single deployment command you can browse to the application running in the container view an app deployed to azure container instances in browser aci app browser include quickstarts free trial note include azure cli prepare your environment md this quickstart requires version 2 0 55 or later of the azure cli if using azure cloud shell the latest version is already installed warning best practice user s credentials passed via command line interface cli are stored as plain text in the backend storing credentials in plain text is a security risk microsoft advises customers to store user credentials in cli environment variables to ensure they are encrypted transformed when stored in the backend create a resource group azure container instances like all azure resources must be deployed into a resource group resource groups allow you to organize and manage related azure resources first create a resource group named myresourcegroup in the eastus location with the az group create az group create command azurecli interactive az group create name myresourcegroup location eastus create a container now that you have a resource group you can run a container in azure to create a container instance with the azure cli provide a resource group name container instance name and docker container image to the az container create az container create command in this</td></tr><tr><td>container-instances/container-instances-quickstart.md</td><td>container-instances</td><td>4590942c3f8226cca172e9dc3d9d9a7d9d3ec44487ec8925019efd2940a29a16</td><td>eastus create a container now that you have a resource group you can run a container in azure to create a container instance with the azure cli provide a resource group name container instance name and docker container image to the az container create az container create command in this quickstart you use the public mcr microsoft com azuredocs aci helloworld image this image packages a small web app written in node js that serves a static html page you can expose your containers to the internet by specifying one or more ports to open a dns name label or both in this quickstart you deploy a container with a dns name label so that the web app is publicly reachable execute a command similar to the following to start a container instance set a dns name label value that s unique within the azure region where you create the instance if you receive a dns name label not available error message try a different dns name label azurecli interactive az container create resource group myresourcegroup name mycontainer image mcr microsoft com azuredocs aci helloworld dns name label aci demo ports 80 os type linux memory 1 5 cpu 1 to deploy the container into a specific availability zone use the zone argument and specify the logical zone number azurecli interactive az container create resource group myresourcegroup name mycontainer image mcr microsoft com azuredocs aci helloworld dns name label aci demo ports 80 os type linux memory 1 5 cpu 1 zone 1 important zonal deployments are only available in regions that support availability zones to see if your region supports availability zones see azure regions list within a few seconds you should get a response from the azure cli indicating the deployment completed check its status with the az container show az container show command azurecli interactive az container show resource group myresourcegroup name mycontainer query fqdn ipaddress fqdn provisioningstate provisioningstate out table when you run the command the container s fully qualified domain name fqdn and its provisioning state are displayed output fqdn provisioningstate aci demo eastus azurecontainer io succeeded if the container s provisioningstate is succeeded go to its fqdn in your browser if you see a web page similar to the following congratulations you successfully deployed an application running in a docker container to azure view an app deployed to azure container instances in browser aci</td></tr><tr><td>container-instances/container-instances-quickstart.md</td><td>container-instances</td><td>efdb721d468c11a3a99e75cc4e8e6035cd5a5f41f5030dbe830b7063ebed29e1</td><td>io succeeded if the container s provisioningstate is succeeded go to its fqdn in your browser if you see a web page similar to the following congratulations you successfully deployed an application running in a docker container to azure view an app deployed to azure container instances in browser aci app browser if at first the application isn t displayed you might need to wait a few seconds while dns propagates then try refreshing your browser pull the container logs when you need to troubleshoot a container or the application it runs or just see its output start by viewing the container instance s logs pull the container instance logs with the az container logs az container logs command azurecli interactive az container logs resource group myresourcegroup name mycontainer the output displays the logs for the container and should show the http get requests generated when you viewed the application in your browser output listening on port 80 ffff 10 240 255 55 21 mar 2019 17 43 53 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 ffff 10 240 255 55 21 mar 2019 17 44 36 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 ffff 10 240 255 55 21 mar 2019 17 44 36 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 attach output streams in addition to viewing the logs you can attach your local standard out and standard error streams to that of the container first execute the az container attach az container attach command to attach your local console to the container s output streams azurecli interactive az container attach resource group myresourcegroup name mycontainer once attached refresh your browser a few times to generate some more output when you re done detach your console with control c you should see output similar to the following sample output container mycontainer is in state running count 1 last timestamp 2019 03 21 17 27 20 00 00 pulling image mcr microsoft com azuredocs aci helloworld count 1 last timestamp 2019 03 21</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "container-instances/container-instances-quickstart.md",
         "container-instances",
         "270cb38d2abc6af424f9c11d46123bbfb03530b32545c638a1b734f1e9767434",
         "title quickstart deploy docker container to container instance azure cli description in this quickstart you use the azure cli to quickly deploy a containerized web app that runs in an isolated azure container instance ms topic quickstart ms author tomcassidy author tomvcassidy ms service azure container instances services container instances ms date 11 17 2025 ms update cycle 180 days ms custom mvc devx track azurecli mode api customer intent as a developer i want to quickly deploy a docker container using the command line so that i can run my web application without managing complex orchestration platforms quickstart deploy a container instance in azure using the azure cli use azure container instances to run serverless docker containers in azure with simplicity and speed deploy an application to a container instance on demand when you don t need a full container orchestration platform like azure kubernetes service in this quickstart you use the azure cli to deploy an isolated docker container and make its application available with a fully qualified domain name fqdn a few seconds after you execute a single deployment command you can browse to the application running in the container view an app deployed to azure container instances in browser aci app browser include quickstarts free trial note include azure cli prepare your environment md this quickstart requires version 2 0 55 or later of the azure cli if using azure cloud shell the latest version is already installed warning best practice user s credentials passed via command line interface cli are stored as plain text in the backend storing credentials in plain text is a security risk microsoft advises customers to store user credentials in cli environment variables to ensure they are encrypted transformed when stored in the backend create a resource group azure container instances like all azure resources must be deployed into a resource group resource groups allow you to organize and manage related azure resources first create a resource group named myresourcegroup in the eastus location with the az group create az group create command azurecli interactive az group create name myresourcegroup location eastus create a container now that you have a resource group you can run a container in azure to create a container instance with the azure cli provide a resource group name container instance name and docker container image to the az container create az container create command in this"
        ],
        [
         "container-instances/container-instances-quickstart.md",
         "container-instances",
         "4590942c3f8226cca172e9dc3d9d9a7d9d3ec44487ec8925019efd2940a29a16",
         "eastus create a container now that you have a resource group you can run a container in azure to create a container instance with the azure cli provide a resource group name container instance name and docker container image to the az container create az container create command in this quickstart you use the public mcr microsoft com azuredocs aci helloworld image this image packages a small web app written in node js that serves a static html page you can expose your containers to the internet by specifying one or more ports to open a dns name label or both in this quickstart you deploy a container with a dns name label so that the web app is publicly reachable execute a command similar to the following to start a container instance set a dns name label value that s unique within the azure region where you create the instance if you receive a dns name label not available error message try a different dns name label azurecli interactive az container create resource group myresourcegroup name mycontainer image mcr microsoft com azuredocs aci helloworld dns name label aci demo ports 80 os type linux memory 1 5 cpu 1 to deploy the container into a specific availability zone use the zone argument and specify the logical zone number azurecli interactive az container create resource group myresourcegroup name mycontainer image mcr microsoft com azuredocs aci helloworld dns name label aci demo ports 80 os type linux memory 1 5 cpu 1 zone 1 important zonal deployments are only available in regions that support availability zones to see if your region supports availability zones see azure regions list within a few seconds you should get a response from the azure cli indicating the deployment completed check its status with the az container show az container show command azurecli interactive az container show resource group myresourcegroup name mycontainer query fqdn ipaddress fqdn provisioningstate provisioningstate out table when you run the command the container s fully qualified domain name fqdn and its provisioning state are displayed output fqdn provisioningstate aci demo eastus azurecontainer io succeeded if the container s provisioningstate is succeeded go to its fqdn in your browser if you see a web page similar to the following congratulations you successfully deployed an application running in a docker container to azure view an app deployed to azure container instances in browser aci"
        ],
        [
         "container-instances/container-instances-quickstart.md",
         "container-instances",
         "efdb721d468c11a3a99e75cc4e8e6035cd5a5f41f5030dbe830b7063ebed29e1",
         "io succeeded if the container s provisioningstate is succeeded go to its fqdn in your browser if you see a web page similar to the following congratulations you successfully deployed an application running in a docker container to azure view an app deployed to azure container instances in browser aci app browser if at first the application isn t displayed you might need to wait a few seconds while dns propagates then try refreshing your browser pull the container logs when you need to troubleshoot a container or the application it runs or just see its output start by viewing the container instance s logs pull the container instance logs with the az container logs az container logs command azurecli interactive az container logs resource group myresourcegroup name mycontainer the output displays the logs for the container and should show the http get requests generated when you viewed the application in your browser output listening on port 80 ffff 10 240 255 55 21 mar 2019 17 43 53 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 ffff 10 240 255 55 21 mar 2019 17 44 36 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 ffff 10 240 255 55 21 mar 2019 17 44 36 0000 get http 1 1 304 mozilla 5 0 windows nt 10 0 win64 x64 applewebkit 537 36 khtml like gecko chrome 72 0 3626 121 safari 537 36 attach output streams in addition to viewing the logs you can attach your local standard out and standard error streams to that of the container first execute the az container attach az container attach command to attach your local console to the container s output streams azurecli interactive az container attach resource group myresourcegroup name mycontainer once attached refresh your browser a few times to generate some more output when you re done detach your console with control c you should see output similar to the following sample output container mycontainer is in state running count 1 last timestamp 2019 03 21 17 27 20 00 00 pulling image mcr microsoft com azuredocs aci helloworld count 1 last timestamp 2019 03 21"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "doc_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT doc_id, category, chunk_id, chunk_text FROM {CHUNKS_TABLE} LIMIT 3\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938eb47e-40fc-47a4-82ff-8dcb6c7efba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+-------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|doc_id                                               |category           |chunk_index|chunk_preview                                                                                                                                                                                           |\n+-----------------------------------------------------+-------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|container-instances/container-instances-quickstart.md|container-instances|0          |title quickstart deploy docker container to container instance azure cli description in this quickstart you use the azure cli to quickly deploy a containerized web app that runs in an isolated azure c|\n|container-instances/container-instances-quickstart.md|container-instances|1          |eastus create a container now that you have a resource group you can run a container in azure to create a container instance with the azure cli provide a resource group name container instance name an|\n|container-instances/container-instances-quickstart.md|container-instances|2          |io succeeded if the container s provisioningstate is succeeded go to its fqdn in your browser if you see a web page similar to the following congratulations you successfully deployed an application ru|\n+-----------------------------------------------------+-------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# chunk_text is too long to show\n",
    "chunks_df.select(\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"chunk_index\",\n",
    "    F.substring(\"chunk_text\", 1, 200).alias(\"chunk_preview\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0a4ebb-7ae3-457b-a30d-c68980f41904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 938 rows\n"
     ]
    }
   ],
   "source": [
    "# Collect chunks in manageable batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "rows = chunks_df.select(\n",
    "    \"chunk_id\",\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"chunk_index\",\n",
    "    \"chunk_text\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Found {len(rows)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fda7c37-f800-44aa-ad49-728fc87f0b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk in row:  0\nChunk in row:  64\nChunk in row:  128\nChunk in row:  192\nChunk in row:  256\nChunk in row:  320\nChunk in row:  384\nChunk in row:  448\nChunk in row:  512\nChunk in row:  576\nChunk in row:  640\nChunk in row:  704\nChunk in row:  768\nChunk in row:  832\nChunk in row:  896\n"
     ]
    }
   ],
   "source": [
    "# Below can run for a while depends on how many chunks\n",
    "\n",
    "# Generate embeddings\n",
    "embedded_rows = []\n",
    "\n",
    "for i in range(0, len(rows), BATCH_SIZE):\n",
    "    print(\"Chunk in row: \", i)\n",
    "    batch = rows[i:i + BATCH_SIZE]\n",
    "    texts = [r.chunk_text for r in batch]\n",
    "\n",
    "    embeddings = embed_texts(texts)\n",
    "\n",
    "    for r, emb in zip(batch, embeddings):\n",
    "        embedded_rows.append((\n",
    "            r.chunk_id,\n",
    "            r.doc_id,\n",
    "            r.category,\n",
    "            r.title,\n",
    "            r.url,\n",
    "            r.chunk_index,\n",
    "            r.chunk_text,\n",
    "            emb\n",
    "        ))\n",
    "\n",
    "    # Below line is used during initial test to limit data: only include first 10 batches\n",
    "    # if i > 10*BATCH_SIZE: break\n",
    "\n",
    "    time.sleep(0.5)  # be polite to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70ff03e-9c18-46e0-93c5-aef7bec7842b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n| dim|\n+----+\n|1536|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings DataFrame\n",
    "\n",
    "embeddings_df = spark.createDataFrame(\n",
    "    embedded_rows,\n",
    "    schema=[\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"category\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"chunk_text\",\n",
    "        \"embedding\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check vector length:\n",
    "embeddings_df.select(F.size(\"embedding\").alias(\"dim\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb060d6-3188-473d-ac3a-47856914f57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    embeddings_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(EMB_TABLE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6164392d-5ba5-41dc-946c-7cb5de5a3015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>938</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         938
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) FROM {EMB_TABLE}\n",
    "\"\"\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db8f318-01fb-4533-9ace-921ec394e50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>embedding_dim</th></tr></thead><tbody><tr><td>container-instances</td><td>1536</td></tr><tr><td>container-instances</td><td>1536</td></tr><tr><td>container-instances</td><td>1536</td></tr><tr><td>container-instances</td><td>1536</td></tr><tr><td>container-instances</td><td>1536</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "container-instances",
         1536
        ],
        [
         "container-instances",
         1536
        ],
        [
         "container-instances",
         1536
        ],
        [
         "container-instances",
         1536
        ],
        [
         "container-instances",
         1536
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "embedding_dim",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT category, size(embedding) AS embedding_dim FROM {EMB_TABLE} LIMIT 5\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_generate_embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}