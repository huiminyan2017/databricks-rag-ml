{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f45fad-0ff1-4175-bf02-24d584238c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 06 - RAG Evaluation\n",
    "\n",
    "This notebook evaluates the quality of our RAG system using an LLM-as-judge approach.\n",
    "We score each query on:\n",
    "- retrieval_relevance (1 - 5): are retrieved chunks relevant to the question?\n",
    "- answer_relevance (1 - 5): does the answer address the question?\n",
    "- faithfulness (1 - 5): is the answer grounded in the retrieved sources?\n",
    "\n",
    "It creates the Delta table, inspects its structure, and demonstrates how to query evaluation metrics. All reusable evaluation logic (LLM-as-judge, scoring, logging) lives in `00_utils.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0d834c-20ec-4e25-80f8-3d6e63181687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Design notes\n",
    "\n",
    "This table enables offline and online quality monitoring of the RAG system.\n",
    "\n",
    "Each query is scored on:\n",
    "- Retrieval relevance — did we fetch the right info?\n",
    "- Answer relevance — did the model answer the question?\n",
    "- Faithfulness — is the answer grounded in the retrieved sources?\n",
    "\n",
    "This allows:\n",
    "- Regression detection\n",
    "- Model comparison\n",
    "- Retriever A/B tests\n",
    "- Prompt iteration tracking\n",
    "- Hallucination analysis\n",
    "\n",
    "Evaluation is intentionally separated from serving so that:\n",
    "- We can batch-evaluate\n",
    "- We can re-score old answers\n",
    "- We can add human labels later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40aa65a-a7fb-4e23-9313-76f4ebb6dc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_install_deps_and_restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052ee830-58c0-466c-b5a0-5a4dbd2e277d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203434b9-6870-4643-9460-bf2c287fdcfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2838c5d-5ba8-41e2-bf98-e251de5630a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create eval question table\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {RAG_EVAL_TABLE} (\n",
    "  evaluation_id STRING,\n",
    "  query_id STRING,\n",
    "  question STRING,\n",
    "  answer STRING,\n",
    "  retrieval_relevance INT,\n",
    "  answer_relevance INT,\n",
    "  faithfulness INT,\n",
    "  evaluator STRING,\n",
    "  notes STRING,\n",
    "  created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acef5f8-26fd-4218-90be-9e99e11d31b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- evaluation_id: string (nullable = true)\n |-- query_id: string (nullable = true)\n |-- question: string (nullable = true)\n |-- answer: string (nullable = true)\n |-- retrieval_relevance: integer (nullable = true)\n |-- answer_relevance: integer (nullable = true)\n |-- faithfulness: integer (nullable = true)\n |-- evaluator: string (nullable = true)\n |-- notes: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages already installed. No restart needed.\n"
     ]
    }
   ],
   "source": [
    "spark.table(RAG_EVAL_TABLE).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb97a9f7-be0e-4808-9a12-82cdfbb5b3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>created_at</th><th>query_id</th><th>retrieval_relevance</th><th>answer_relevance</th><th>faithfulness</th><th>evaluator</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "created_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "query_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retrieval_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "answer_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "faithfulness",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "evaluator",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect recent evaluations\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  created_at,\n",
    "  query_id,\n",
    "  retrieval_relevance,\n",
    "  answer_relevance,\n",
    "  faithfulness,\n",
    "  evaluator\n",
    "FROM {RAG_EVAL_TABLE}\n",
    "ORDER BY created_at DESC\n",
    "LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f0410e-8fa8-4250-9bfe-d65f44466e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>n</th><th>avg_retrieval</th><th>avg_answer</th><th>avg_faithfulness</th></tr></thead><tbody><tr><td>0</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "n",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_retrieval",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_answer",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_faithfulness",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate metrics\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  count(*) AS n,\n",
    "  avg(retrieval_relevance) AS avg_retrieval,\n",
    "  avg(answer_relevance) AS avg_answer,\n",
    "  avg(faithfulness) AS avg_faithfulness\n",
    "FROM {RAG_EVAL_TABLE}\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815e083c-b5b7-467e-a66f-abd97656b753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day</th><th>avg_retrieval</th><th>avg_answer</th><th>avg_faithfulness</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "day",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "avg_retrieval",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_answer",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_faithfulness",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trend over time\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  date_trunc('day', created_at) AS day,\n",
    "  avg(retrieval_relevance) AS avg_retrieval,\n",
    "  avg(answer_relevance) AS avg_answer,\n",
    "  avg(faithfulness) AS avg_faithfulness\n",
    "FROM {RAG_EVAL_TABLE}\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ceb190-be2e-41de-baa2-41dd79e6948a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>created_at</th><th>question</th><th>retrieval_relevance</th><th>answer_relevance</th><th>faithfulness</th><th>top_source</th><th>notes</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "created_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "question",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retrieval_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "answer_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "faithfulness",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "top_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "notes",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join logs + evaluations (root cause analysis)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  e.created_at,\n",
    "  l.question,\n",
    "  e.retrieval_relevance,\n",
    "  e.answer_relevance,\n",
    "  e.faithfulness,\n",
    "  l.retrieved_chunks[0].url AS top_source,\n",
    "  e.notes\n",
    "FROM {RAG_EVAL_TABLE} e\n",
    "JOIN {RAG_LOG_TABLE} l\n",
    "  ON e.query_id = l.query_id\n",
    "ORDER BY e.created_at DESC\n",
    "LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe79d24-590a-4a55-bef3-a4c5bd1e3784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_init_openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29186e4c-778a-4f9d-ab77-a060f29880ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated: e26f88a9-6388-4875-be1d-2852a31fe1d8 {'retrieval_relevance': 2, 'answer_relevance': 5, 'faithfulness': 3, 'notes': 'The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not directly support the answer, leading to some concerns about faithfulness.'}\nEvaluated: 6bb2bad3-7cc5-4805-b332-6fa0ffb75219 {'retrieval_relevance': 2, 'answer_relevance': 5, 'faithfulness': 5, 'notes': 'The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not contain relevant information directly related to the differences between normal and ephemeral VMs, which affects the retrieval relevance score.'}\nEvaluated: 0ff22b09-ebc9-49e9-b32f-cf25a2c4ca6e {'retrieval_relevance': 2, 'answer_relevance': 5, 'faithfulness': 4, 'notes': 'The retrieved excerpts do not directly address the differences between normal Azure VMs and ephemeral VMs, which affects the relevance of the retrieval. However, the answer provided is comprehensive and accurately describes the differences based on general knowledge of Azure VMs. The answer is mostly faithful to the context of ephemeral VMs, but it lacks direct support from the excerpts.'}\nEvaluated: 0e2132c9-3324-4f02-b24d-a4dc628bc924 {'retrieval_relevance': 2, 'answer_relevance': 5, 'faithfulness': 3, 'notes': 'The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question well. However, the retrieved excerpts do not directly support the answer, leading to a lower faithfulness score. The excerpts focus on specific VM series and configurations without directly discussing the differences between normal and ephemeral VMs.'}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-837f26e601634d52914a048833188c73\", \"tr-89d4e4935cca4bccb9951b033f6ffb0d\", \"tr-d875238fb73b4d66aa9beaa89bdce308\", \"tr-5cd48139636d4dec855b586af394f31e\"]",
      "text/plain": [
       "[Trace(request_id=tr-837f26e601634d52914a048833188c73), Trace(request_id=tr-89d4e4935cca4bccb9951b033f6ffb0d), Trace(request_id=tr-d875238fb73b4d66aa9beaa89bdce308), Trace(request_id=tr-5cd48139636d4dec855b586af394f31e)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def enrich_chunks_with_text(retrieved_chunks_py: list[dict]) -> list[dict]:\n",
    "    chunk_ids = [c[\"chunk_id\"] for c in retrieved_chunks_py if c.get(\"chunk_id\")]\n",
    "    if not chunk_ids:\n",
    "        return retrieved_chunks_py\n",
    "\n",
    "    text_map = (\n",
    "        chunks_df\n",
    "        .where(F.col(\"chunk_id\").isin(chunk_ids))\n",
    "        .select(\"chunk_id\", \"chunk_text\")\n",
    "        .collect()\n",
    "    )\n",
    "    text_map = {r[\"chunk_id\"]: r[\"chunk_text\"] for r in text_map}\n",
    "\n",
    "    for c in retrieved_chunks_py:\n",
    "        cid = c.get(\"chunk_id\")\n",
    "        c[\"chunk_text\"] = text_map.get(cid, \"\")\n",
    "    return retrieved_chunks_py\n",
    "\n",
    "# Nowerun evaluation properly:\n",
    "logs_df = spark.table(RAG_LOG_TABLE)\n",
    "\n",
    "recent_logs_5 = (\n",
    "    logs_df\n",
    "    .orderBy(F.col(\"created_at\").desc())\n",
    "    .limit(5)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "chunks_df = spark.table(CHUNKS_TABLE).select(\"chunk_id\", \"chunk_text\")\n",
    "\n",
    "for r in recent_logs_5:\n",
    "    query_id = r[\"query_id\"]\n",
    "    question = r[\"question\"]\n",
    "    answer = r[\"answer\"]\n",
    "\n",
    "    retrieved_chunks_py = []\n",
    "    for c in r[\"retrieved_chunks\"]:\n",
    "        retrieved_chunks_py.append({\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"doc_id\": c[\"doc_id\"],\n",
    "            \"title\": c[\"title\"],\n",
    "            \"url\": c[\"url\"],\n",
    "            \"chunk_index\": c[\"chunk_index\"],\n",
    "            \"category\": c[\"category\"],\n",
    "            \"score\": c[\"score\"],\n",
    "            # Your log table schema for retrieved_chunks does not include chunk_text, so the judge has no real evidence to assess faithfulness.\n",
    "            \"chunk_text\": \"\" # enrich below\n",
    "        })\n",
    "\n",
    "    retrieved_chunks_py = enrich_chunks_with_text(retrieved_chunks_py)\n",
    "\n",
    "    scores = judge_rag(question, answer, retrieved_chunks_py)\n",
    "    eval_id = write_evaluation(query_id, question, answer, scores, evaluator=\"llm_judge_v1_with_text\")\n",
    "\n",
    "    print(\"Evaluated:\", query_id, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d324c5-14cf-4a2d-bef3-305f95f737f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>n</th><th>avg_retrieval_relevance</th><th>avg_answer_relevance</th><th>avg_faithfulness</th></tr></thead><tbody><tr><td>4</td><td>2.0</td><td>5.0</td><td>3.75</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         2.0,
         5.0,
         3.75
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "n",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_retrieval_relevance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_answer_relevance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_faithfulness",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View evaluation summary in SQL\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  count(*) AS n,\n",
    "  avg(retrieval_relevance) AS avg_retrieval_relevance,\n",
    "  avg(answer_relevance) AS avg_answer_relevance,\n",
    "  avg(faithfulness) AS avg_faithfulness\n",
    "FROM {RAG_EVAL_TABLE}\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fc7bac-4330-45b6-924e-76299ff0db01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>created_at</th><th>query_id</th><th>retrieval_relevance</th><th>answer_relevance</th><th>faithfulness</th><th>notes</th></tr></thead><tbody><tr><td>2026-01-11T23:26:35.219874Z</td><td>0e2132c9-3324-4f02-b24d-a4dc628bc924</td><td>2</td><td>5</td><td>3</td><td>The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question well. However, the retrieved excerpts do not directly support the answer, leading to a lower faithfulness score. The excerpts focus on specific VM series and configurations without directly discussing the differences between normal and ephemeral VMs.</td></tr><tr><td>2026-01-11T23:26:31.821616Z</td><td>0ff22b09-ebc9-49e9-b32f-cf25a2c4ca6e</td><td>2</td><td>5</td><td>4</td><td>The retrieved excerpts do not directly address the differences between normal Azure VMs and ephemeral VMs, which affects the relevance of the retrieval. However, the answer provided is comprehensive and accurately describes the differences based on general knowledge of Azure VMs. The answer is mostly faithful to the context of ephemeral VMs, but it lacks direct support from the excerpts.</td></tr><tr><td>2026-01-11T23:26:28.125017Z</td><td>6bb2bad3-7cc5-4805-b332-6fa0ffb75219</td><td>2</td><td>5</td><td>5</td><td>The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not contain relevant information directly related to the differences between normal and ephemeral VMs, which affects the retrieval relevance score.</td></tr><tr><td>2026-01-11T23:26:24.138362Z</td><td>e26f88a9-6388-4875-be1d-2852a31fe1d8</td><td>2</td><td>5</td><td>3</td><td>The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not directly support the answer, leading to some concerns about faithfulness.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-11T23:26:35.219874Z",
         "0e2132c9-3324-4f02-b24d-a4dc628bc924",
         2,
         5,
         3,
         "The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question well. However, the retrieved excerpts do not directly support the answer, leading to a lower faithfulness score. The excerpts focus on specific VM series and configurations without directly discussing the differences between normal and ephemeral VMs."
        ],
        [
         "2026-01-11T23:26:31.821616Z",
         "0ff22b09-ebc9-49e9-b32f-cf25a2c4ca6e",
         2,
         5,
         4,
         "The retrieved excerpts do not directly address the differences between normal Azure VMs and ephemeral VMs, which affects the relevance of the retrieval. However, the answer provided is comprehensive and accurately describes the differences based on general knowledge of Azure VMs. The answer is mostly faithful to the context of ephemeral VMs, but it lacks direct support from the excerpts."
        ],
        [
         "2026-01-11T23:26:28.125017Z",
         "6bb2bad3-7cc5-4805-b332-6fa0ffb75219",
         2,
         5,
         5,
         "The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not contain relevant information directly related to the differences between normal and ephemeral VMs, which affects the retrieval relevance score."
        ],
        [
         "2026-01-11T23:26:24.138362Z",
         "e26f88a9-6388-4875-be1d-2852a31fe1d8",
         2,
         5,
         3,
         "The answer provides a detailed comparison between normal Azure VMs and ephemeral VMs, addressing the question effectively. However, the retrieved excerpts do not directly support the answer, leading to some concerns about faithfulness."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "created_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "query_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retrieval_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "answer_relevance",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "faithfulness",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "notes",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show latest evaluations:\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  created_at,\n",
    "  query_id,\n",
    "  retrieval_relevance,\n",
    "  answer_relevance,\n",
    "  faithfulness,\n",
    "  notes\n",
    "FROM {RAG_EVAL_TABLE}\n",
    "ORDER BY created_at DESC\n",
    "LIMIT 20\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_rag_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}