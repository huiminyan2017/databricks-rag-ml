{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec5a1b4e-da0a-49e9-9197-a88fabe69628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have one row per chunk now. In this notebook we will convert each chunk's text into a numeric vector (embedding) so later can be used for: search by semantic similarity, get the retrieve relevant chunks and feed them to an LLM for RAG\n",
    " \n",
    "It reads chunk-level documents from Unity Catalog, generates text embeddings using an external embedding model, and stores the resulting vectors as a Delta table for semantic search and retrieval-augmented generation (RAG).\n",
    "\n",
    "Input and Output:\n",
    "- Input table: databricks_rag_demo.default.azure_compute_doc_chunks\n",
    "- Output table: databricks_rag_demo.default.azure_compute_doc_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deafaf35-daba-4967-a20b-1f960bc77db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Embedding strategy (important decisions)\n",
    "\n",
    "For this project we will:\n",
    "-   Use OpenAI-style embeddings (works with OpenAI or Azure OpenAI)\n",
    "-   Generate embeddings in batches (not per row)\n",
    "- Store embeddings as: ARRAY<FLOAT> (simple, portable)\n",
    "- Keep metadata alongside vectors\n",
    "\n",
    "This is the most common production pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f62e409-33ff-4a0b-b46f-077a7f2e0a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b195736-c588-4176-b3a2-82ce9cdeeaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks_df = spark.table(\n",
    "    \"databricks_rag_demo.default.azure_compute_doc_chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b744fae-fbb4-4bbc-b859-e3fad5f321a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>doc_id</th><th>category</th><th>chunk_id</th><th>chunk_text</th></tr></thead><tbody><tr><td>virtual-machines/extensions/salt-minion.md</td><td>virtual-machines</td><td>8c083b147e62c8532ca84e73b344b0079e8b0cadebffb92a9edfd1546f525b8a</td><td>title salt minion for linux or windows azure vms description install salt minion on linux or windows vms using the vm extension ms topic concept article ms service azure virtual machines ms subservice extensions ms custom devx track arm template devx track azurecli devx track terraform linux related content ms author gabsta author gabstamsft ms date 08 18 2025 customer intent as a cloud administrator i want to install salt minion on my azure vms using vm extensions so that i can effectively manage and automate configurations across my infrastructure install salt minion on linux or windows vms using the vm extension prerequisites a microsoft azure account with one or more windows or linux vms a salt master either on premises or in a cloud that can accept connections from salt minions hosted on azure the salt minion vm extension requires that the target vm is connected to the internet in order to fetch salt packages include vm assist troubleshooting tools supported platforms azure vm running any of the following supported os ubuntu 20 04 22 04 x86_64 debian 10 11 x86_64 oracle linux 7 8 9 x86_64 rhel 7 8 9 x86_64 microsoft windows 10 11 pro x86_64 microsoft windows server 2012 r2 2016 2019 2022 datacenter x86_64 if you want another distro to be supported assuming salt supports it an issue can be filed on gitlab supported salt minion versions 3006 and up onedir extension details publisher name turtletraction oss linux extension name salt minion linux windows extension name salt minion windows salt minion settings master_address salt master address to connect to localhost by default minion_id minion id hostname by default salt_version salt minion version to install for example 3006 1 latest by default install salt minion using the azure portal 1 select one of your vms 2 in the left menu click extensions applications 3 click add 4 in the gallery type salt minion in the search bar 5 select the salt minion tile and click next 6 enter configuration parameters in the provided form see salt minion settings 7 click review create install salt minion using the azure cli to uninstall it install salt minion using the azure arm template install salt minion using terraform assuming that you have defined a vm resource in terraform named vm_ubuntu then use something like this to install the extension on it support for commercial support or assistance with salt</td></tr><tr><td>virtual-machines/extensions/salt-minion.md</td><td>virtual-machines</td><td>b31b8f47afa253eb5a55d07e79f3ae3aec9ba1cb892b4161ca98873280c7eff0</td><td>the azure cli to uninstall it install salt minion using the azure arm template install salt minion using terraform assuming that you have defined a vm resource in terraform named vm_ubuntu then use something like this to install the extension on it support for commercial support or assistance with salt you can visit the extension creator turtletraction the source code of this extension is available on gitlab for azure related issues you can file an azure support incident go to the azure support site and select get support</td></tr><tr><td>virtual-machines/extensions/backup-azure-sql-server-running-azure-vm.md</td><td>virtual-machines</td><td>e81e730867bab0e1924516a758e63f50decf7d4ec97fe49e4e0640fe734eb34a</td><td>title azure backup for sql server running in azure vm description in this article learn how to register azure backup in sql server running in an azure virtual machine ms topic concept article ms service azure virtual machines ms subservice extensions ms author gabsta ms reviewer jushiman author gabstamsft ms collection windows ms date 08 18 2025 customer intent as a database administrator i want to register azure backup for my sql server running in an azure vm so that i can ensure reliable backup and recovery of my database workloads azure backup for sql server running in azure vm azure backup amongst other offerings provides support for backing up workloads such as sql server running in azure vms since the sql application is running within an azure vm the backup service needs permission to access the application and fetch the necessary details to do that azure backup installs the azurebackupwindowsworkload extension on the vm in which the sql server is running during the registration process triggered by the user include vm assist troubleshooting tools prerequisites for the list of supported scenarios refer to the supportability matrix supported by azure backup network connectivity azure backup supports nsg tags deploying a proxy server or listed ip ranges for details on each of the methods refer this article extension schema the extension schema and property values are the configuration values runtime settings that service is passing to crp api these config values are used during registration and upgrade azurebackupwindowsworkload extension also uses this schema the schema is pre set a new parameter can be added in the objectstr field the following json shows the schema for the workloadbackup extension property values name value example data type locale en us string taskid 1c0ae461 9d3b 418c a505 bb31dfe2095d string objectstr br publicsettings eyjjb250ywluzxjqcm9wzxj0awvzijp7iknvbnrhaw5lckleijoimzvjmjqxytitogrjny00zge5lwi4ntmtmjdjytjhndzlm2zkiiwiswrnz210q29udgfpbmvyswqiojm0nty3odg5lcjszxnvdxjjzulkijoimdu5nwiwogetyzi4zi00zmfllwe5oditotkwowmymgvjnjvhiiwiu3vic2nyaxb0aw9uswqioijkngezotliny1iyjayltq2mwmtoddmys1jntm5odi3ztgzntqilcjvbmlxdwvdb250ywluzxjoyw1lijoiodm4mdzjodutntq4os00nmnhlweyztctnwmznznhyjg3otcyin0sinn0yw1wtglzdci6w3siu2vydmljzu5hbwuiojusilnlcnzpy2vtdgftcfvybci6imh0dha6xc9cl015v0xgywjtdmmuy29tin1dfq string commandstarttimeutcticks 636967192566036845 string vmtype microsoft compute virtualmachines string objectstr br protectedsettings eyjjb250ywluzxjqcm9wzxj0awvzijp7iknvbnrhaw5lckleijoimzvjmjqxytitogrjny00zge5lwi4ntmtmjdjytjhndzlm2zkiiwiswrnz210q29udgfpbmvyswqiojm0nty3odg5lcjszxnvdxjjzulkijoimdu5nwiwogetyzi4zi00zmfllwe5oditotkwowmymgvjnjvhiiwiu3vic2nyaxb0aw9uswqioijkngezotliny1iyjayltq2mwmtoddmys1jntm5odi3ztgzntqilcjvbmlxdwvdb250ywluzxjoyw1lijoiodm4mdzjodutntq4os00nmnhlweyztctnwmznznhyjg3otcyin0sinn0yw1wtglzdci6w3siu2vydmljzu5hbwuiojusilnlcnzpy2vtdgftcfvybci6imh0dha6xc9cl015v0xgywjtdmmuy29tin1dfq string logsbloburi span https span seapod01coord1exsapk732 blob core windows net bcdrextensionlogs 111111111 1111 1111 1111 111111111111 vmubuntu1404ltsc v2 logs txt sv 2014 02 14 sr b sig dbwyhwfeac5yjzisgxokk 2fewqq2ao1vs1e0rdw 2flsbw 3d st 2017 11 09t14 3a33 3a29z se 2017 11 09t17 3a38 3a29z sp rw string statusbloburi span https span seapod01coord1exsapk732 blob core windows net bcdrextensionlogs 111111111 1111 1111 1111 111111111111 vmubuntu1404ltsc v2 status txt sv 2014 02 14 sr b sig 96rzbptkcjmv7qfexm5idub 2filktwgblwbwg6ih96ao 3d st 2017 11 09t14 3a33 3a29z se 2017 11 09t17 3a38 3a29z sp</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "virtual-machines/extensions/salt-minion.md",
         "virtual-machines",
         "8c083b147e62c8532ca84e73b344b0079e8b0cadebffb92a9edfd1546f525b8a",
         "title salt minion for linux or windows azure vms description install salt minion on linux or windows vms using the vm extension ms topic concept article ms service azure virtual machines ms subservice extensions ms custom devx track arm template devx track azurecli devx track terraform linux related content ms author gabsta author gabstamsft ms date 08 18 2025 customer intent as a cloud administrator i want to install salt minion on my azure vms using vm extensions so that i can effectively manage and automate configurations across my infrastructure install salt minion on linux or windows vms using the vm extension prerequisites a microsoft azure account with one or more windows or linux vms a salt master either on premises or in a cloud that can accept connections from salt minions hosted on azure the salt minion vm extension requires that the target vm is connected to the internet in order to fetch salt packages include vm assist troubleshooting tools supported platforms azure vm running any of the following supported os ubuntu 20 04 22 04 x86_64 debian 10 11 x86_64 oracle linux 7 8 9 x86_64 rhel 7 8 9 x86_64 microsoft windows 10 11 pro x86_64 microsoft windows server 2012 r2 2016 2019 2022 datacenter x86_64 if you want another distro to be supported assuming salt supports it an issue can be filed on gitlab supported salt minion versions 3006 and up onedir extension details publisher name turtletraction oss linux extension name salt minion linux windows extension name salt minion windows salt minion settings master_address salt master address to connect to localhost by default minion_id minion id hostname by default salt_version salt minion version to install for example 3006 1 latest by default install salt minion using the azure portal 1 select one of your vms 2 in the left menu click extensions applications 3 click add 4 in the gallery type salt minion in the search bar 5 select the salt minion tile and click next 6 enter configuration parameters in the provided form see salt minion settings 7 click review create install salt minion using the azure cli to uninstall it install salt minion using the azure arm template install salt minion using terraform assuming that you have defined a vm resource in terraform named vm_ubuntu then use something like this to install the extension on it support for commercial support or assistance with salt"
        ],
        [
         "virtual-machines/extensions/salt-minion.md",
         "virtual-machines",
         "b31b8f47afa253eb5a55d07e79f3ae3aec9ba1cb892b4161ca98873280c7eff0",
         "the azure cli to uninstall it install salt minion using the azure arm template install salt minion using terraform assuming that you have defined a vm resource in terraform named vm_ubuntu then use something like this to install the extension on it support for commercial support or assistance with salt you can visit the extension creator turtletraction the source code of this extension is available on gitlab for azure related issues you can file an azure support incident go to the azure support site and select get support"
        ],
        [
         "virtual-machines/extensions/backup-azure-sql-server-running-azure-vm.md",
         "virtual-machines",
         "e81e730867bab0e1924516a758e63f50decf7d4ec97fe49e4e0640fe734eb34a",
         "title azure backup for sql server running in azure vm description in this article learn how to register azure backup in sql server running in an azure virtual machine ms topic concept article ms service azure virtual machines ms subservice extensions ms author gabsta ms reviewer jushiman author gabstamsft ms collection windows ms date 08 18 2025 customer intent as a database administrator i want to register azure backup for my sql server running in an azure vm so that i can ensure reliable backup and recovery of my database workloads azure backup for sql server running in azure vm azure backup amongst other offerings provides support for backing up workloads such as sql server running in azure vms since the sql application is running within an azure vm the backup service needs permission to access the application and fetch the necessary details to do that azure backup installs the azurebackupwindowsworkload extension on the vm in which the sql server is running during the registration process triggered by the user include vm assist troubleshooting tools prerequisites for the list of supported scenarios refer to the supportability matrix supported by azure backup network connectivity azure backup supports nsg tags deploying a proxy server or listed ip ranges for details on each of the methods refer this article extension schema the extension schema and property values are the configuration values runtime settings that service is passing to crp api these config values are used during registration and upgrade azurebackupwindowsworkload extension also uses this schema the schema is pre set a new parameter can be added in the objectstr field the following json shows the schema for the workloadbackup extension property values name value example data type locale en us string taskid 1c0ae461 9d3b 418c a505 bb31dfe2095d string objectstr br publicsettings eyjjb250ywluzxjqcm9wzxj0awvzijp7iknvbnrhaw5lckleijoimzvjmjqxytitogrjny00zge5lwi4ntmtmjdjytjhndzlm2zkiiwiswrnz210q29udgfpbmvyswqiojm0nty3odg5lcjszxnvdxjjzulkijoimdu5nwiwogetyzi4zi00zmfllwe5oditotkwowmymgvjnjvhiiwiu3vic2nyaxb0aw9uswqioijkngezotliny1iyjayltq2mwmtoddmys1jntm5odi3ztgzntqilcjvbmlxdwvdb250ywluzxjoyw1lijoiodm4mdzjodutntq4os00nmnhlweyztctnwmznznhyjg3otcyin0sinn0yw1wtglzdci6w3siu2vydmljzu5hbwuiojusilnlcnzpy2vtdgftcfvybci6imh0dha6xc9cl015v0xgywjtdmmuy29tin1dfq string commandstarttimeutcticks 636967192566036845 string vmtype microsoft compute virtualmachines string objectstr br protectedsettings eyjjb250ywluzxjqcm9wzxj0awvzijp7iknvbnrhaw5lckleijoimzvjmjqxytitogrjny00zge5lwi4ntmtmjdjytjhndzlm2zkiiwiswrnz210q29udgfpbmvyswqiojm0nty3odg5lcjszxnvdxjjzulkijoimdu5nwiwogetyzi4zi00zmfllwe5oditotkwowmymgvjnjvhiiwiu3vic2nyaxb0aw9uswqioijkngezotliny1iyjayltq2mwmtoddmys1jntm5odi3ztgzntqilcjvbmlxdwvdb250ywluzxjoyw1lijoiodm4mdzjodutntq4os00nmnhlweyztctnwmznznhyjg3otcyin0sinn0yw1wtglzdci6w3siu2vydmljzu5hbwuiojusilnlcnzpy2vtdgftcfvybci6imh0dha6xc9cl015v0xgywjtdmmuy29tin1dfq string logsbloburi span https span seapod01coord1exsapk732 blob core windows net bcdrextensionlogs 111111111 1111 1111 1111 111111111111 vmubuntu1404ltsc v2 logs txt sv 2014 02 14 sr b sig dbwyhwfeac5yjzisgxokk 2fewqq2ao1vs1e0rdw 2flsbw 3d st 2017 11 09t14 3a33 3a29z se 2017 11 09t17 3a38 3a29z sp rw string statusbloburi span https span seapod01coord1exsapk732 blob core windows net bcdrextensionlogs 111111111 1111 1111 1111 111111111111 vmubuntu1404ltsc v2 status txt sv 2014 02 14 sr b sig 96rzbptkcjmv7qfexm5idub 2filktwgblwbwg6ih96ao 3d st 2017 11 09t14 3a33 3a29z se 2017 11 09t17 3a38 3a29z sp"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "doc_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "chunk_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "chunk_text",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 10
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "doc_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT doc_id, category, chunk_id, chunk_text FROM databricks_rag_demo.default.azure_compute_doc_chunks LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938eb47e-40fc-47a4-82ff-8dcb6c7efba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|doc_id                                                                 |category        |chunk_index|chunk_preview                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|virtual-machines/extensions/salt-minion.md                             |virtual-machines|0          |title salt minion for linux or windows azure vms description install salt minion on linux or windows vms using the vm extension ms topic concept article ms service azure virtual machines ms subservice|\n",
      "|virtual-machines/extensions/salt-minion.md                             |virtual-machines|1          |the azure cli to uninstall it install salt minion using the azure arm template install salt minion using terraform assuming that you have defined a vm resource in terraform named vm_ubuntu then use so|\n",
      "|virtual-machines/extensions/backup-azure-sql-server-running-azure-vm.md|virtual-machines|0          |title azure backup for sql server running in azure vm description in this article learn how to register azure backup in sql server running in an azure virtual machine ms topic concept article ms servi|\n",
      "+-----------------------------------------------------------------------+----------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# chunk_text is too long to show\n",
    "chunks_df.select(\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"chunk_index\",\n",
    "    F.substring(\"chunk_text\", 1, 200).alias(\"chunk_preview\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4031b183-7bc1-4e8c-9e5d-958203429643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How to create Azure OpenAI service and get a api key:\n",
    "- In Azure Portal: Create Azure OpenAI resource\n",
    "- Deploy a model: text-embedding-3-small or text-embedding-ada-002\n",
    "- Get:\n",
    "\t- Endpoint\n",
    "\t- API key\n",
    "\n",
    "\n",
    "How to store API key in notebook (one-time)\n",
    "- Workspace → Secrets\n",
    "- Create scope: openai\n",
    "  - Key: OPENAI_API_KEY\n",
    "  - Value: your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceebace-4da7-45d3-8f2a-6e7e5692218f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in /databricks/python3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/361.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: tqdm, sniffio, jiter, h11, anyio, httpcore, httpx, openai\n",
      "Successfully installed anyio-4.12.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 openai-2.14.0 sniffio-1.3.1 tqdm-4.67.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8617cbc-70bc-4fb4-aafb-150b4fb4204b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Initialize OpenAI emebdding client\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3327405c-1371-4522-89ac-fdd0f26d89f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We do NOT query embed for one row at a time, We do in batches for cost and performance.\n",
    "\n",
    "def embed_texts(texts, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=texts\n",
    "    )\n",
    "    return [d.embedding for d in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0a4ebb-7ae3-457b-a30d-c68980f41904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5758 rows\n"
     ]
    }
   ],
   "source": [
    "# Collect chunks in manageable batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "rows = chunks_df.select(\n",
    "    \"chunk_id\",\n",
    "    \"doc_id\",\n",
    "    \"category\",\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"chunk_index\",\n",
    "    \"chunk_text\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Found {len(rows)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fda7c37-f800-44aa-ad49-728fc87f0b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-a7ed08f1d27942d79d688b840fc99724\", \"tr-70e3746dea274d2e95f4f5ed6242f7a0\", \"tr-f21c8bb53c1f458c88737fdd2b89f93b\", \"tr-d76a90fbfcbf4076b48659a92c2fb3d2\", \"tr-b9ca28528ec14c4facf6f4459e8e55d8\", \"tr-df2459b6e0bf4be681ceca60f0e082b5\", \"tr-b6bff65d7cab4c70bacb75c113a2468a\", \"tr-0cfe7b59f735447faf204ce4fe52e9e5\", \"tr-01168658d0c54ad099d7160a2c3d222a\", \"tr-d4384a44be9543e9808ec32dae219f4f\"]",
      "text/plain": [
       "[Trace(request_id=tr-a7ed08f1d27942d79d688b840fc99724), Trace(request_id=tr-70e3746dea274d2e95f4f5ed6242f7a0), Trace(request_id=tr-f21c8bb53c1f458c88737fdd2b89f93b), Trace(request_id=tr-d76a90fbfcbf4076b48659a92c2fb3d2), Trace(request_id=tr-b9ca28528ec14c4facf6f4459e8e55d8), Trace(request_id=tr-df2459b6e0bf4be681ceca60f0e082b5), Trace(request_id=tr-b6bff65d7cab4c70bacb75c113a2468a), Trace(request_id=tr-0cfe7b59f735447faf204ce4fe52e9e5), Trace(request_id=tr-01168658d0c54ad099d7160a2c3d222a), Trace(request_id=tr-d4384a44be9543e9808ec32dae219f4f)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Generate embeddings\n",
    "\n",
    "embedded_rows = []\n",
    "\n",
    "for i in range(0, len(rows), BATCH_SIZE):\n",
    "    print(f\"{i}\")\n",
    "    batch = rows[i:i + BATCH_SIZE]\n",
    "    texts = [r.chunk_text for r in batch]\n",
    "\n",
    "    embeddings = embed_texts(texts)\n",
    "\n",
    "    for r, emb in zip(batch, embeddings):\n",
    "        embedded_rows.append((\n",
    "            r.chunk_id,\n",
    "            r.doc_id,\n",
    "            r.category,\n",
    "            r.title,\n",
    "            r.url,\n",
    "            r.chunk_index,\n",
    "            r.chunk_text,\n",
    "            emb\n",
    "        ))\n",
    "\n",
    "        # first 10 batches\n",
    "    if i > 10*BATCH_SIZE: break\n",
    "\n",
    "    time.sleep(0.5)  # be polite to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70ff03e-9c18-46e0-93c5-aef7bec7842b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| dim|\n",
      "+----+\n",
      "|1536|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings DataFrame\n",
    "\n",
    "embeddings_df = spark.createDataFrame(\n",
    "    embedded_rows,\n",
    "    schema=[\n",
    "        \"chunk_id\",\n",
    "        \"doc_id\",\n",
    "        \"category\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"chunk_index\",\n",
    "        \"chunk_text\",\n",
    "        \"embedding\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check vector length:\n",
    "embeddings_df.select(F.size(\"embedding\").alias(\"dim\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb060d6-3188-473d-ac3a-47856914f57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    embeddings_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\n",
    "        \"databricks_rag_demo.default.azure_compute_doc_embeddings\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6164392d-5ba5-41dc-946c-7cb5de5a3015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>768</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         768
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "__autoGeneratedAlias": "true"
            },
            "name": "count(1)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM databricks_rag_demo.default.azure_compute_doc_embeddings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1db8f318-01fb-4533-9ace-921ec394e50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT category, size(embedding) AS embedding_dim FROM databricks_rag_demo.default.azure_compute_doc_embeddings LIMIT 5;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8945701984265515,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_generate_embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
